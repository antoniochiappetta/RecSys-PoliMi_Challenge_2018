{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "388f1b60ec94ae8e893ccd1ebc8af9b874ab1784"
   },
   "source": [
    "# DEF - Magic to load Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "8bb213f3a1bff1062ec358b2a9c40433535cc1f0"
   },
   "outputs": [],
   "source": [
    "%reload_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72f330f45f047688443d666333eb37a82d284d04"
   },
   "source": [
    "# DEF - Imports - Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "ec3ae410eb846b2945bc61d1c0b9f7d6c7454892"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import time, sys\n",
    "import subprocess\n",
    "\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7197f94bd5d021cfdedd025c8518a36e79f6a76d"
   },
   "source": [
    "# DEF - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "e22fad4e9256cca236495126ea843de7c4bd3ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tracks.csv', 'submission.csv', 'train.csv', 'target_playlists.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import subprocess\n",
    "import time, sys, copy\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "def precision(is_relevant, n_test_items):\n",
    "\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / min(n_test_items, len(is_relevant))\n",
    "\n",
    "    assert 0 <= precision_score <= 1, precision_score\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "def recall(is_relevant, pos_items):\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / pos_items.shape[0]\n",
    "\n",
    "    assert 0 <= recall_score <= 1, recall_score\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def map(is_relevant, pos_items):\n",
    "\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    map_score = np.sum(p_at_k) / np.min([pos_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    assert 0 <= map_score <= 1, map_score\n",
    "    return map_score\n",
    "\n",
    "class EvaluatorMetrics(Enum):\n",
    "\n",
    "    PRECISION = \"PRECISION\"\n",
    "    RECALL = \"RECALL\"\n",
    "    MAP = \"MAP\"\n",
    "\n",
    "def create_empty_metrics_dict(n_items, n_users, URM_train, ignore_items, ignore_users, cutoff, diversity_similarity_object):\n",
    "\n",
    "    empty_dict = {}\n",
    "    \n",
    "    for metric in EvaluatorMetrics:\n",
    "        empty_dict[metric.value] = 0.0\n",
    "\n",
    "    return  empty_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"Abstract Evaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"Evaluator_Base_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                        diversity_object = None,\n",
    "                        ignore_items = None,\n",
    "                        ignore_users = None):\n",
    "\n",
    "        super(Evaluator, self).__init__()\n",
    "\n",
    "\n",
    "\n",
    "        if ignore_items is None:\n",
    "            self.ignore_items_flag = False\n",
    "            self.ignore_items_ID = np.array([])\n",
    "        else:\n",
    "            print(\"Ignoring {} Items\".format(len(ignore_items)))\n",
    "            self.ignore_items_flag = True\n",
    "            self.ignore_items_ID = np.array(ignore_items)\n",
    "\n",
    "        self.cutoff_list = cutoff_list.copy()\n",
    "        self.max_cutoff = max(self.cutoff_list)\n",
    "\n",
    "        self.minRatingsPerUser = minRatingsPerUser\n",
    "        self.exclude_seen = exclude_seen\n",
    "\n",
    "        if not isinstance(URM_test_list, list):\n",
    "            self.URM_test = URM_test_list.copy()\n",
    "            URM_test_list = [URM_test_list]\n",
    "        else:\n",
    "            raise ValueError(\"List of URM_test not supported\")\n",
    "\n",
    "        self.diversity_object = diversity_object\n",
    "\n",
    "        self.n_users = URM_test_list[0].shape[0]\n",
    "        self.n_items = URM_test_list[0].shape[1]\n",
    "\n",
    "        # Prune users with an insufficient number of ratings\n",
    "        # During testing CSR is faster\n",
    "        self.URM_test_list = []\n",
    "        usersToEvaluate_mask = np.zeros(self.n_users, dtype=np.bool)\n",
    "\n",
    "        for URM_test in URM_test_list:\n",
    "\n",
    "            URM_test = sps.csr_matrix(URM_test)\n",
    "            self.URM_test_list.append(URM_test)\n",
    "\n",
    "            rows = URM_test.indptr\n",
    "            numRatings = np.ediff1d(rows)\n",
    "            new_mask = numRatings >= minRatingsPerUser\n",
    "\n",
    "            usersToEvaluate_mask = np.logical_or(usersToEvaluate_mask, new_mask)\n",
    "\n",
    "        self.usersToEvaluate = np.arange(self.n_users)[usersToEvaluate_mask]\n",
    "\n",
    "\n",
    "        if ignore_users is not None:\n",
    "            print(\"Ignoring {} Users\".format(len(ignore_users)))\n",
    "            self.ignore_users_ID = np.array(ignore_users)\n",
    "            self.usersToEvaluate = set(self.usersToEvaluate) - set(ignore_users)\n",
    "        else:\n",
    "            self.ignore_users_ID = np.array([])\n",
    "\n",
    "\n",
    "        self.usersToEvaluate = list(self.usersToEvaluate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a Recommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"The method evaluateRecommender not implemented for this evaluator class\")\n",
    "\n",
    "\n",
    "\n",
    "    def get_user_relevant_items(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in getting relevant items\"\n",
    "\n",
    "        return self.URM_test.indices[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id+1]]\n",
    "\n",
    "\n",
    "    def get_user_test_ratings(self, user_id):\n",
    "\n",
    "        assert self.URM_test.getformat() == \"csr\", \"Evaluator_Base_Class: URM_test is not CSR, this will cause errors in relevant items ratings\"\n",
    "\n",
    "        return self.URM_test.data[self.URM_test.indptr[user_id]:self.URM_test.indptr[user_id+1]]\n",
    "\n",
    "\n",
    "\n",
    "    def get_result_string(self, results_run):\n",
    "\n",
    "        output_str = \"\"\n",
    "\n",
    "        for cutoff in results_run.keys():\n",
    "\n",
    "            results_run_current_cutoff = results_run[cutoff]\n",
    "\n",
    "            output_str += \"CUTOFF: {} - \".format(cutoff)\n",
    "\n",
    "            for metric in results_run_current_cutoff.keys():\n",
    "                output_str += \"{}: {:.7f}, \".format(metric, results_run_current_cutoff[metric])\n",
    "\n",
    "            output_str += \"\\n\"\n",
    "\n",
    "        return output_str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _run_evaluation_on_selected_users(self, recommender_object, usersToEvaluate):\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print = time.time()\n",
    "\n",
    "\n",
    "        results_dict = {}\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.URM_train,\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "        n_users_evaluated = 0\n",
    "\n",
    "\n",
    "        for test_user in usersToEvaluate:\n",
    "\n",
    "            # Being the URM CSR, the indices are the non-zero column indexes\n",
    "            relevant_items = self.get_user_relevant_items(test_user)\n",
    "\n",
    "            n_users_evaluated += 1\n",
    "\n",
    "            recommended_items = recommender_object.recommend(test_user, remove_seen_flag=self.exclude_seen,\n",
    "                                                             cutoff = self.max_cutoff, remove_top_pop_flag=False, remove_CustomItems_flag=self.ignore_items_flag)\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                results_current_cutoff[\"PRECISION\"]            += precision(is_relevant_current_cutoff, len(relevant_items))\n",
    "                results_current_cutoff[\"RECALL\"]               += recall(is_relevant_current_cutoff, relevant_items)\n",
    "                results_current_cutoff[\"MAP\"]                  += map(is_relevant_current_cutoff, relevant_items)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print > 30 or n_users_evaluated==len(self.usersToEvaluate):\n",
    "                print(\"SequentialEvaluator: Processed {} ( {:.2f}% ) in {:.2f} seconds. Users per second: {:.0f}\".format(\n",
    "                              n_users_evaluated,\n",
    "                              100.0* float(n_users_evaluated)/len(self.usersToEvaluate),\n",
    "                              time.time()-start_time,\n",
    "                              float(n_users_evaluated)/(time.time()-start_time)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        return results_dict, n_users_evaluated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SequentialEvaluator(Evaluator):\n",
    "    \"\"\"SequentialEvaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"SequentialEvaluator_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object = None,\n",
    "                 ignore_items = None,\n",
    "                 ignore_users = None):\n",
    "\n",
    "\n",
    "        super(SequentialEvaluator, self).__init__(URM_test_list, cutoff_list,\n",
    "                            diversity_object = diversity_object,\n",
    "                            minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                            ignore_items = ignore_items, ignore_users = ignore_users)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _run_evaluation_on_selected_users(self, recommender_object, usersToEvaluate, block_size = 1000):\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print = time.time()\n",
    "\n",
    "\n",
    "        results_dict = {}\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.get_URM_train(),\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "        n_users_evaluated = 0\n",
    "\n",
    "        # Start from -block_size to ensure it to be 0 at the first block\n",
    "        user_batch_start = 0\n",
    "        user_batch_end = 0\n",
    "\n",
    "        while user_batch_start < len(self.usersToEvaluate):\n",
    "\n",
    "            user_batch_end = user_batch_start + block_size\n",
    "            user_batch_end = min(user_batch_end, len(usersToEvaluate))\n",
    "\n",
    "            test_user_batch_array = np.array(usersToEvaluate[user_batch_start:user_batch_end])\n",
    "            user_batch_start = user_batch_end\n",
    "\n",
    "            # Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\n",
    "            recommended_items_batch_list = recommender_object.recommend(test_user_batch_array,\n",
    "                                                                  remove_seen_flag=self.exclude_seen,\n",
    "                                                                  cutoff = self.max_cutoff,\n",
    "                                                                  remove_top_pop_flag=False,\n",
    "                                                                  remove_CustomItems_flag=self.ignore_items_flag)\n",
    "\n",
    "\n",
    "            # Compute recommendation quality for each user in batch\n",
    "            for batch_user_index in range(len(recommended_items_batch_list)):\n",
    "\n",
    "                user_id = test_user_batch_array[batch_user_index]\n",
    "                recommended_items = recommended_items_batch_list[batch_user_index]\n",
    "\n",
    "                # Being the URM CSR, the indices are the non-zero column indexes\n",
    "                relevant_items = self.get_user_relevant_items(user_id)\n",
    "                is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "                n_users_evaluated += 1\n",
    "\n",
    "                for cutoff in self.cutoff_list:\n",
    "\n",
    "                    results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                    is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                    recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                    results_current_cutoff[\"PRECISION\"]            += precision(is_relevant_current_cutoff, len(relevant_items))\n",
    "                    results_current_cutoff[\"RECALL\"]               += recall(is_relevant_current_cutoff, relevant_items)\n",
    "                    results_current_cutoff[\"MAP\"]                  += map(is_relevant_current_cutoff, relevant_items)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if time.time() - start_time_print > 30 or n_users_evaluated==len(self.usersToEvaluate):\n",
    "                    print(\"SequentialEvaluator: Processed {} ( {:.2f}% ) in {:.2f} seconds. Users per second: {:.0f}\".format(\n",
    "                                  n_users_evaluated,\n",
    "                                  100.0* float(n_users_evaluated)/len(self.usersToEvaluate),\n",
    "                                  time.time()-start_time,\n",
    "                                  float(n_users_evaluated)/(time.time()-start_time)))\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "                    start_time_print = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        return results_dict, n_users_evaluated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a Recommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "\n",
    "\n",
    "        results_dict, n_users_evaluated = self._run_evaluation_on_selected_users(recommender_object, self.usersToEvaluate)\n",
    "\n",
    "\n",
    "        if (n_users_evaluated > 0):\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                for key in results_current_cutoff.keys():\n",
    "\n",
    "                    value = results_current_cutoff[key]\n",
    "\n",
    "                    results_current_cutoff[key] = value/n_users_evaluated\n",
    "\n",
    "                precision_ = results_current_cutoff[\"PRECISION\"]\n",
    "                recall_ = results_current_cutoff[\"RECALL\"]\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "\n",
    "\n",
    "\n",
    "        results_run_string = self.get_result_string(results_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "\n",
    "        return (results_dict, results_run_string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "class _ParallelEvaluator_batch(Evaluator):\n",
    "    \"\"\"SequentialEvaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"SequentialEvaluator_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object = None,\n",
    "                 ignore_items = None,\n",
    "                 ignore_users = None):\n",
    "\n",
    "\n",
    "        super(_ParallelEvaluator_batch, self).__init__(URM_test_list, cutoff_list,\n",
    "                            diversity_object = diversity_object,\n",
    "                            minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                            ignore_items = ignore_items, ignore_users = ignore_users)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a Recommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        results_dict, n_users_evaluated = self._run_evaluation_on_selected_users(recommender_object, self.usersToEvaluate)\n",
    "\n",
    "        return (results_dict, n_users_evaluated)\n",
    "\n",
    "\n",
    "\n",
    "def _run_parallel_evaluator(evaluator_object, recommender_object):\n",
    "\n",
    "    results_dict, _ = evaluator_object.evaluateRecommender(recommender_object)\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "\n",
    "def _merge_results_dict(results_dict_1, results_dict_2, n_users_2):\n",
    "\n",
    "    assert results_dict_1.keys() == results_dict_2.keys(), \"_merge_results_dict: the two result dictionaries have different cutoff values\"\n",
    "\n",
    "\n",
    "    merged_dict = copy.deepcopy(results_dict_1)\n",
    "\n",
    "    for cutoff in merged_dict.keys():\n",
    "\n",
    "        merged_dict_cutoff = merged_dict[cutoff]\n",
    "        results_dict_2_cutoff = results_dict_2[cutoff]\n",
    "\n",
    "        for key in merged_dict_cutoff.keys():\n",
    "\n",
    "            result_metric = merged_dict_cutoff[key]\n",
    "\n",
    "            merged_dict_cutoff[key] = result_metric + results_dict_2_cutoff[key]*n_users_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ParallelEvaluator(Evaluator):\n",
    "    \"\"\"ParallelEvaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"ParallelEvaluator_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object = None,\n",
    "                 ignore_items = None,\n",
    "                 ignore_users = None):\n",
    "\n",
    "        assert False, \"ParallelEvaluator is not a stable implementation\"\n",
    "\n",
    "        super(ParallelEvaluator, self).__init__(URM_test_list, cutoff_list,\n",
    "                            diversity_object = diversity_object,\n",
    "                            minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                            ignore_items = ignore_items, ignore_users = ignore_users)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object, n_processes = None):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a Recommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "        if n_processes is None:\n",
    "            n_processes = int(multiprocessing.cpu_count()/2)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        # Split the users to evaluate\n",
    "        n_processes = min(n_processes, len(self.usersToEvaluate))\n",
    "        batch_len = int(len(self.usersToEvaluate)/n_processes)\n",
    "        batch_len = max(batch_len, 1)\n",
    "\n",
    "        sequential_evaluators_list = []\n",
    "        sequential_evaluators_n_users_list = []\n",
    "\n",
    "        for n_evaluator in range(n_processes):\n",
    "\n",
    "            stat_user = n_evaluator*batch_len\n",
    "            end_user = min((n_evaluator+1)*batch_len, len(self.usersToEvaluate))\n",
    "\n",
    "            if n_evaluator == n_processes-1:\n",
    "                end_user = len(self.usersToEvaluate)\n",
    "\n",
    "\n",
    "            batch_users = self.usersToEvaluate[stat_user:end_user]\n",
    "            sequential_evaluators_n_users_list.append(len(batch_users))\n",
    "\n",
    "            not_in_batch_users = np.in1d(self.usersToEvaluate, batch_users, invert=True)\n",
    "            not_in_batch_users = np.array(self.usersToEvaluate)[not_in_batch_users]\n",
    "\n",
    "            new_evaluator = _ParallelEvaluator_batch(self.URM_test, self.cutoff_list, ignore_users=not_in_batch_users)\n",
    "\n",
    "            sequential_evaluators_list.append(new_evaluator)\n",
    "\n",
    "\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "\n",
    "        run_parallel_evaluator_partial = partial(_run_parallel_evaluator, recommender_object = recommender_object)\n",
    "\n",
    "        pool = multiprocessing.Pool(processes = n_processes, maxtasksperchild=1)\n",
    "        resultList = pool.map(run_parallel_evaluator_partial, sequential_evaluators_list)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"ParallelEvaluator: Processed {} ( {:.2f}% ) in {:.2f} seconds. Users per second: {:.0f}\".format(\n",
    "                      len(self.usersToEvaluate),\n",
    "                      100.0* float(len(self.usersToEvaluate))/len(self.usersToEvaluate),\n",
    "                      time.time()-start_time,\n",
    "                      float(len(self.usersToEvaluate))/(time.time()-start_time)))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        sys.stderr.flush()\n",
    "\n",
    "\n",
    "\n",
    "        results_dict = {}\n",
    "        n_users_evaluated = 0\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "             results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.URM_train,\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "\n",
    "        for new_result_index in range(len(resultList)):\n",
    "\n",
    "            new_result, n_users_evaluated_batch = resultList[new_result_index]\n",
    "            n_users_evaluated += n_users_evaluated_batch\n",
    "\n",
    "            results_dict = _merge_results_dict(results_dict, new_result, n_users_evaluated_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            for key in results_dict[cutoff].keys():\n",
    "                results_dict[cutoff][key] /= len(self.usersToEvaluate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if n_users_evaluated > 0:\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                for key in results_current_cutoff.keys():\n",
    "\n",
    "                    value = results_current_cutoff[key]\n",
    "\n",
    "                    results_current_cutoff[key] = value/n_users_evaluated\n",
    "\n",
    "                precision_ = results_current_cutoff[\"PRECISION\"]\n",
    "                recall_ = results_current_cutoff[\"RECALL\"]\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sequential_evaluators_list = None\n",
    "        sequential_evaluators_n_users_list = None\n",
    "\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "\n",
    "\n",
    "        results_run_string = self.get_result_string(results_dict)\n",
    "\n",
    "        return (results_dict, results_run_string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LeaveOneOutEvaluator(Evaluator):\n",
    "    \"\"\"SequentialEvaluator\"\"\"\n",
    "\n",
    "    EVALUATOR_NAME = \"LeaveOneOutEvaluator_Class\"\n",
    "\n",
    "    def __init__(self, URM_test_list, URM_test_negative, cutoff_list, minRatingsPerUser=1, exclude_seen=True,\n",
    "                 diversity_object = None,\n",
    "                 ignore_items = None,\n",
    "                 ignore_users = None):\n",
    "        \"\"\"\n",
    "        :param URM_test_list:\n",
    "        :param URM_test_negative: Items to rank together with the test items\n",
    "        :param cutoff_list:\n",
    "        :param minRatingsPerUser:\n",
    "        :param exclude_seen:\n",
    "        :param diversity_object:\n",
    "        :param ignore_items:\n",
    "        :param ignore_users:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        super(LeaveOneOutEvaluator, self).__init__(URM_test_list, cutoff_list,\n",
    "                            diversity_object = diversity_object,\n",
    "                            minRatingsPerUser=minRatingsPerUser, exclude_seen=exclude_seen,\n",
    "                            ignore_items = ignore_items, ignore_users = ignore_users)\n",
    "\n",
    "\n",
    "        self.URM_test_negative = sps.csr_matrix(URM_test_negative)\n",
    "\n",
    "\n",
    "\n",
    "    def user_specific_remove_items(self, recommender_object, user_id):\n",
    "\n",
    "        self.ignore_items_flag = True\n",
    "\n",
    "        self._global_ignore_items_ID = self.ignore_items_ID.copy()\n",
    "\n",
    "        #items_to_remove_for_user = self.__all_items.copy()\n",
    "        items_to_remove_for_user_mask = self.__all_items_mask.copy()\n",
    "\n",
    "        ### ADD negative samples\n",
    "        start_pos = self.URM_test_negative.indptr[user_id]\n",
    "        end_pos = self.URM_test_negative.indptr[user_id+1]\n",
    "\n",
    "        items_to_remove_for_user_mask[self.URM_test_negative.indices[start_pos:end_pos]] = False\n",
    "\n",
    "        ### ADD positive samples\n",
    "        start_pos = self.URM_test.indptr[user_id]\n",
    "        end_pos = self.URM_test.indptr[user_id+1]\n",
    "\n",
    "        items_to_remove_for_user_mask[self.URM_test.indices[start_pos:end_pos]] = False\n",
    "\n",
    "        recommender_object.set_items_to_ignore(self.__all_items[items_to_remove_for_user_mask])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommender(self, recommender_object):\n",
    "        \"\"\"\n",
    "        :param recommender_object: the trained recommender object, a Recommender subclass\n",
    "        :param URM_test_list: list of URMs to test the recommender against, or a single URM object\n",
    "        :param cutoff_list: list of cutoffs to be use to report the scores, or a single cutoff\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        results_dict = {}\n",
    "\n",
    "        for cutoff in self.cutoff_list:\n",
    "            results_dict[cutoff] = create_empty_metrics_dict(self.n_items, self.n_users,\n",
    "                                                             recommender_object.URM_train,\n",
    "                                                             self.ignore_items_ID,\n",
    "                                                             self.ignore_users_ID,\n",
    "                                                             cutoff,\n",
    "                                                             self.diversity_object)\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print = time.time()\n",
    "\n",
    "        n_eval = 0\n",
    "\n",
    "        self.__all_items = np.arange(0, self.n_items, dtype=np.int)\n",
    "        self.__all_items = set(self.__all_items)\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.set_items_to_ignore(self.ignore_items_ID)\n",
    "\n",
    "\n",
    "\n",
    "        for test_user in self.usersToEvaluate:\n",
    "\n",
    "            # Being the URM CSR, the indices are the non-zero column indexes\n",
    "            relevant_items = self.get_user_relevant_items(test_user)\n",
    "\n",
    "            n_eval += 1\n",
    "\n",
    "            self.user_specific_remove_items(recommender_object, test_user)\n",
    "\n",
    "            # recommended_items = recommender_object.recommend(np.array(test_user), remove_seen_flag=self.exclude_seen,\n",
    "            #                                                  cutoff = self.max_cutoff, remove_top_pop_flag=False, remove_CustomItems_flag=self.ignore_items_flag)\n",
    "            recommended_items = recommender_object.recommend(np.atleast_1d(test_user),\n",
    "                                                              remove_seen_flag=self.exclude_seen,\n",
    "                                                              cutoff = self.max_cutoff,\n",
    "                                                              remove_top_pop_flag=False,\n",
    "                                                              remove_CustomItems_flag=self.ignore_items_flag)\n",
    "\n",
    "            recommended_items = np.array(recommended_items[0])\n",
    "\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                is_relevant_current_cutoff = is_relevant[0:cutoff]\n",
    "                recommended_items_current_cutoff = recommended_items[0:cutoff]\n",
    "\n",
    "                results_current_cutoff[\"PRECISION\"]            += precision(is_relevant_current_cutoff, len(relevant_items))\n",
    "                results_current_cutoff[\"RECALL\"]               += recall(is_relevant_current_cutoff, relevant_items)\n",
    "                results_current_cutoff[\"MAP\"]                  += map(is_relevant_current_cutoff, relevant_items)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print > 30 or n_eval==len(self.usersToEvaluate):\n",
    "                print(\"SequentialEvaluator: Processed {} ( {:.2f}% ) in {:.2f} seconds. Users per second: {:.0f}\".format(\n",
    "                              n_eval,\n",
    "                              100.0* float(n_eval)/len(self.usersToEvaluate),\n",
    "                              time.time()-start_time,\n",
    "                              float(n_eval)/(time.time()-start_time)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print = time.time()\n",
    "\n",
    "\n",
    "        if (n_eval > 0):\n",
    "\n",
    "            for cutoff in self.cutoff_list:\n",
    "\n",
    "                results_current_cutoff = results_dict[cutoff]\n",
    "\n",
    "                for key in results_current_cutoff.keys():\n",
    "\n",
    "                    value = results_current_cutoff[key]\n",
    "\n",
    "                    results_current_cutoff[key] = value/n_eval\n",
    "\n",
    "                precision_ = results_current_cutoff[\"PRECISION\"]\n",
    "                recall_ = results_current_cutoff[\"RECALL\"]\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING: No users had a sufficient number of relevant items\")\n",
    "\n",
    "\n",
    "        if self.ignore_items_flag:\n",
    "            recommender_object.reset_items_to_ignore()\n",
    "\n",
    "\n",
    "\n",
    "        results_run_string = self.get_result_string(results_dict)\n",
    "\n",
    "        return (results_dict, results_run_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d075cc85982c727d5a40245d4bac82af2751d8f1"
   },
   "source": [
    "# DEF - Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "0c991828100c50902a0bd6817e1dc1ca5381d596"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Recommender(object):\n",
    "    \"\"\"Abstract Recommender\"\"\"\n",
    "\n",
    "    RECOMMENDER_NAME = \"Recommender_Base_Class\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Recommender, self).__init__()\n",
    "\n",
    "        self.URM_train = None\n",
    "        self.sparse_weights = True\n",
    "        self.normalize = False\n",
    "\n",
    "        self.filterTopPop = False\n",
    "        self.filterTopPop_ItemsID = np.array([], dtype=np.int)\n",
    "\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        return self.URM_train.copy()\n",
    "\n",
    "\n",
    "    def set_items_to_ignore(self, items_to_ignore):\n",
    "\n",
    "        self.items_to_ignore_flag = True\n",
    "        self.items_to_ignore_ID = np.array(items_to_ignore, dtype=np.int)\n",
    "\n",
    "    def reset_items_to_ignore(self):\n",
    "\n",
    "        self.items_to_ignore_flag = False\n",
    "        self.items_to_ignore_ID = np.array([], dtype=np.int)\n",
    "\n",
    "\n",
    "    def _remove_TopPop_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.filterTopPop_ItemsID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_CustomItems_on_scores(self, scores_batch):\n",
    "        scores_batch[:, self.items_to_ignore_ID] = -np.inf\n",
    "        return scores_batch\n",
    "\n",
    "\n",
    "    def _remove_seen_on_scores(self, user_id, scores):\n",
    "\n",
    "        assert self.URM_train.getformat() == \"csr\", \"Recommender_Base_Class: URM_train is not CSR, this will cause errors in filtering seen items\"\n",
    "\n",
    "        seen = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "\n",
    "        scores[seen] = -np.inf\n",
    "        return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_item_score(self, user_id):\n",
    "        raise NotImplementedError(\"Recommender: compute_item_score not assigned for current recommender, unable to compute prediction scores\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def recommend(self, user_id_array, cutoff = None, remove_seen_flag=True, remove_top_pop_flag = False, remove_CustomItems_flag = False):\n",
    "\n",
    "        # If is a scalar transform it in a 1-cell array\n",
    "        if np.isscalar(user_id_array):\n",
    "            user_id_array = np.atleast_1d(user_id_array)\n",
    "            single_user = True\n",
    "        else:\n",
    "            single_user = False\n",
    "\n",
    "\n",
    "        if cutoff is None:\n",
    "            cutoff = self.URM_train.shape[1] - 1\n",
    "\n",
    "        # Compute the scores using the model-specific function\n",
    "        # Vectorize over all users in user_id_array\n",
    "        scores_batch = self.compute_item_score(user_id_array)\n",
    "\n",
    "\n",
    "        # if self.normalize:\n",
    "        #     # normalization will keep the scores in the same range\n",
    "        #     # of value of the ratings in dataset\n",
    "        #     user_profile = self.URM_train[user_id]\n",
    "        #\n",
    "        #     rated = user_profile.copy()\n",
    "        #     rated.data = np.ones_like(rated.data)\n",
    "        #     if self.sparse_weights:\n",
    "        #         den = rated.dot(self.W_sparse).toarray().ravel()\n",
    "        #     else:\n",
    "        #         den = rated.dot(self.W).ravel()\n",
    "        #     den[np.abs(den) < 1e-6] = 1.0  # to avoid NaNs\n",
    "        #     scores /= den\n",
    "\n",
    "\n",
    "        for user_index in range(len(user_id_array)):\n",
    "\n",
    "            user_id = user_id_array[user_index]\n",
    "\n",
    "            if remove_seen_flag:\n",
    "                scores_batch[user_index,:] = self._remove_seen_on_scores(user_id, scores_batch[user_index, :])\n",
    "\n",
    "            # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "            # - Partition the data to extract the set of relevant items\n",
    "            # - Sort only the relevant items\n",
    "            # - Get the original item index\n",
    "            # relevant_items_partition = (-scores_user).argpartition(cutoff)[0:cutoff]\n",
    "            # relevant_items_partition_sorting = np.argsort(-scores_user[relevant_items_partition])\n",
    "            # ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "            #\n",
    "            # ranking_list.append(ranking)\n",
    "\n",
    "\n",
    "        if remove_top_pop_flag:\n",
    "            scores_batch = self._remove_TopPop_on_scores(scores_batch)\n",
    "\n",
    "        if remove_CustomItems_flag:\n",
    "            scores_batch = self._remove_CustomItems_on_scores(scores_batch)\n",
    "\n",
    "        # scores_batch = np.arange(0,3260).reshape((1, -1))\n",
    "        # scores_batch = np.repeat(scores_batch, 1000, axis = 0)\n",
    "\n",
    "        # relevant_items_partition is block_size x cutoff\n",
    "        relevant_items_partition = (-scores_batch).argpartition(cutoff, axis=1)[:,0:cutoff]\n",
    "\n",
    "        # Get original value and sort it\n",
    "        # [:, None] adds 1 dimension to the array, from (block_size,) to (block_size,1)\n",
    "        # This is done to correctly get scores_batch value as [row, relevant_items_partition[row,:]]\n",
    "        relevant_items_partition_original_value = scores_batch[np.arange(scores_batch.shape[0])[:, None], relevant_items_partition]\n",
    "        relevant_items_partition_sorting = np.argsort(-relevant_items_partition_original_value, axis=1)\n",
    "        ranking = relevant_items_partition[np.arange(relevant_items_partition.shape[0])[:, None], relevant_items_partition_sorting]\n",
    "\n",
    "        ranking_list = ranking.tolist()\n",
    "\n",
    "\n",
    "        # Return single list for one user, instead of list of lists\n",
    "        if single_user:\n",
    "            ranking_list = ranking_list[0]\n",
    "\n",
    "        return ranking_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluateRecommendations(self, URM_test, at=5, minRatingsPerUser=1, exclude_seen=True,\n",
    "                                filterCustomItems = np.array([], dtype=np.int),\n",
    "                                filterCustomUsers = np.array([], dtype=np.int)):\n",
    "        \"\"\"\n",
    "        Speed info:\n",
    "        - Sparse weighgs: batch mode is 2x faster than sequential\n",
    "        - Dense weighgts: batch and sequential speed are equivalent\n",
    "        :param URM_test:            URM to be used for testing\n",
    "        :param at: 5                    Length of the recommended items\n",
    "        :param minRatingsPerUser: 1     Users with less than this number of interactions will not be evaluated\n",
    "        :param exclude_seen: True       Whether to remove already seen items from the recommended items\n",
    "        :param mode: 'sequential', 'parallel', 'batch'\n",
    "        :param filterTopPop: False or decimal number        Percentage of items to be removed from recommended list and testing interactions\n",
    "        :param filterCustomItems: Array, default empty           Items ID to NOT take into account when recommending\n",
    "        :param filterCustomUsers: Array, default empty           Users ID to NOT take into account when recommending\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        import warnings\n",
    "\n",
    "        warnings.warn(\"DEPRECATED! Use Base.Evaluation.SequentialEvaluator.evaluateRecommendations()\", DeprecationWarning)\n",
    "\n",
    "\n",
    "        # from Base.Evaluation.Evaluator import SequentialEvaluator\n",
    "\n",
    "        evaluator = SequentialEvaluator(URM_test, [at], exclude_seen= exclude_seen,\n",
    "                                        minRatingsPerUser=minRatingsPerUser,\n",
    "                                        ignore_items=filterCustomItems, ignore_users=filterCustomUsers)\n",
    "\n",
    "        results_run, results_run_string = evaluator.evaluateRecommender(self)\n",
    "\n",
    "        results_run = results_run[at]\n",
    "\n",
    "        results_run_lowercase = {}\n",
    "\n",
    "        for key in results_run.keys():\n",
    "            results_run_lowercase[key.lower()] = results_run[key]\n",
    "\n",
    "\n",
    "        return results_run_lowercase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def saveModel(self, folder_path, file_name = None):\n",
    "        raise NotImplementedError(\"Recommender: saveModel not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def loadModel(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        print(\"{}: Loading model from file '{}'\".format(self.RECOMMENDER_NAME, folder_path + file_name))\n",
    "\n",
    "\n",
    "        data_dict = pickle.load(open(folder_path + file_name, \"rb\"))\n",
    "\n",
    "        for attrib_name in data_dict.keys():\n",
    "             self.__setattr__(attrib_name, data_dict[attrib_name])\n",
    "\n",
    "\n",
    "        print(\"{}: Loading complete\".format(self.RECOMMENDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84acbf614d9ab23e1be4dabfe40f5384e12986c1"
   },
   "source": [
    "# DEF - Similarity Matrix Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "fbc8a8297d25df761316f27e3209743c87be5e83"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimilarityMatrixRecommender(object):\n",
    "    \"\"\"\n",
    "    This class refers to a Recommender KNN which uses a similarity matrix, it provides two function to compute item's score\n",
    "    bot for user-based and Item-based models as well as a function to save the W_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimilarityMatrixRecommender, self).__init__()\n",
    "\n",
    "        self.sparse_weights = True\n",
    "\n",
    "        self.compute_item_score = self.compute_score_slim_and_hybrid\n",
    "\n",
    "\n",
    "\n",
    "    def compute_score_item_based(self, user_id):\n",
    "\n",
    "        if self.sparse_weights:\n",
    "            user_profile = self.URM_train[user_id]\n",
    "\n",
    "            return user_profile.dot(self.W_sparse).toarray()\n",
    "\n",
    "        else:\n",
    "\n",
    "            assert False\n",
    "\n",
    "            user_profile = self.URM_train.indices[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "            user_ratings = self.URM_train.data[self.URM_train.indptr[user_id]:self.URM_train.indptr[user_id + 1]]\n",
    "\n",
    "            relevant_weights = self.W[user_profile]\n",
    "            return relevant_weights.T.dot(user_ratings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_score_user_based(self, user_id):\n",
    "\n",
    "        if self.sparse_weights:\n",
    "\n",
    "            return self.W_sparse[user_id].dot(self.URM_train).toarray()\n",
    "\n",
    "        else:\n",
    "            # Numpy dot does not recognize sparse matrices, so we must\n",
    "            # invoke the dot function on the sparse one\n",
    "            return self.URM_train.T.dot(self.W[user_id])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def compute_score_slim_and_hybrid(self, user_id):\n",
    "        \n",
    "        if self.sparse_weights:\n",
    "        \n",
    "            # playlist_profile_u = self.W_sparse_ucf[user_id]\n",
    "            # playlist_profile_i = self.URM_train[user_id]\n",
    "            # scores_ucf = playlist_profile_u.dot(self.URM_train).toarray()\n",
    "            # scores_icf = playlist_profile_i.dot(self.W_sparse_icf).toarray()\n",
    "            # scores_icf_slim = playlist_profile_i.dot(self.W_sparse).toarray()\n",
    "            # scores_icbf = playlist_profile_i.dot(self.W_sparse_icbf).toarray()\n",
    "\n",
    "            # scores_norm_ucf = (scores_ucf-scores_ucf.min() + 1e-6)/(scores_ucf.max()-scores_ucf.min() + 1e-6)\n",
    "            # scores_norm_icf = (scores_icf-scores_icf.min() + 1e-6)/(scores_icf.max()-scores_icf.min() + 1e-6)\n",
    "            # scores_norm_icf_slim = (scores_icf_slim-scores_icf_slim.min() + 1e-6)/(scores_icf_slim.max()-scores_icf_slim.min() + 1e-6)\n",
    "            # scores_norm_icbf = (scores_icbf-scores_icbf.min() + 1e-6)/(scores_icbf.max()-scores_icbf.min() + 1e-6)\n",
    "            \n",
    "            playlist_profile_u = self.W_sparse_ucf[user_id]\n",
    "            playlist_profile_i = self.URM_train[user_id]\n",
    "            scores_ucf = playlist_profile_u.dot(self.URM_train).toarray()\n",
    "            scores_item= playlist_profile_i.dot(self.W_sparse_item).toarray()\n",
    "            scores_icf_slim = playlist_profile_i.dot(self.W_sparse).toarray()\n",
    "            scores_icf = playlist_profile_i.dot(self.W_sparse_icf).toarray()\n",
    "            scores_icbf = playlist_profile_i.dot(self.W_sparse_icbf).toarray()\n",
    "\n",
    "            scores_norm_ucf = (scores_ucf-np.mean(scores_ucf))/(np.std(scores_ucf))\n",
    "            scores_norm_item = (scores_item-np.mean(scores_item))/(np.std(scores_item))\n",
    "            scores_norm_icf_slim = (scores_icf_slim-np.mean(scores_icf_slim))/(np.std(scores_icf_slim))\n",
    "            scores_norm_icf = (scores_icf-np.mean(scores_icf))/(np.std(scores_icf))\n",
    "            scores_norm_icbf = (scores_icbf-np.mean(scores_icbf))/(np.std(scores_icbf))\n",
    "\n",
    "            #return scores_norm_ucf * 0.4 + scores_norm_item * 0.4 + scores_norm_icf_slim * 0.2\n",
    "            return scores_norm_ucf * 0.4 + scores_norm_icf * 0.25 + scores_norm_icf_slim * 0.20 + scores_norm_icbf * 0.15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def saveModel(self, folder_path, file_name = None):\n",
    "\n",
    "        if file_name is None:\n",
    "            file_name = self.RECOMMENDER_NAME\n",
    "\n",
    "        print(\"{}: Saving model in file '{}'\".format(self.RECOMMENDER_NAME, folder_path + file_name))\n",
    "\n",
    "        dictionary_to_save = {\"sparse_weights\": self.sparse_weights}\n",
    "\n",
    "\n",
    "        if self.sparse_weights:\n",
    "            dictionary_to_save[\"W_sparse\"] = self.W_sparse\n",
    "\n",
    "        else:\n",
    "            dictionary_to_save[\"W\"] = self.W\n",
    "\n",
    "\n",
    "        pickle.dump(dictionary_to_save,\n",
    "                    open(folder_path + file_name, \"wb\"),\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "        print(\"{}: Saving complete\".format(self.RECOMMENDER_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9af8b0e272fef91ad49875cc2714eff291bc955"
   },
   "source": [
    "# DEF - Similarity Computation - Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "9fb135db04dcd838300752f9c65bd1f54765448b"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import time, sys\n",
    "import subprocess\n",
    "\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            idx_sorted = np.argsort(column_data)  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[top_k_idx])\n",
    "            rows_indices.extend(column_row_index[top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n",
    "cdef class Compute_Similarity_Cython:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_columns, n_rows\n",
    "\n",
    "    cdef double[:] this_item_weights\n",
    "    cdef int[:] this_item_weights_mask, this_item_weights_id\n",
    "    cdef int this_item_weights_counter\n",
    "\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "    cdef double[:] sumOfSquared, sumOfSquared_to_1_minus_alpha, sumOfSquared_to_alpha\n",
    "    cdef int shrink, normalize, adjusted_cosine, pearson_correlation, tanimoto_coefficient, asymmetric_cosine, dice_coefficient, tversky_coefficient\n",
    "    cdef float asymmetric_alpha, tversky_alpha, tversky_beta\n",
    "\n",
    "    cdef int use_row_weights\n",
    "    cdef double[:] row_weights\n",
    "\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    def __init__(self, dataMatrix, topK = 100, shrink=0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Cython, self).__init__()\n",
    "\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "        self.TopK = min(topK, self.n_columns)\n",
    "        self.this_item_weights = np.zeros(self.n_columns, dtype=np.float64)\n",
    "        self.this_item_weights_id = np.zeros(self.n_columns, dtype=np.int32)\n",
    "        self.this_item_weights_mask = np.zeros(self.n_columns, dtype=np.int32)\n",
    "        self.this_item_weights_counter = 0\n",
    "\n",
    "        # Copy data to avoid altering the original object\n",
    "        dataMatrix = dataMatrix.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            dataMatrix = self.applyAdjustedCosine(dataMatrix)\n",
    "        elif self.pearson_correlation:\n",
    "            dataMatrix = self.applyPearsonCorrelation(dataMatrix)\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            dataMatrix = self.useOnlyBooleanInteractions(dataMatrix)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        self.sumOfSquared = np.array(dataMatrix.power(2).sum(axis=0), dtype=np.float64).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            self.sumOfSquared = np.sqrt(self.sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            self.sumOfSquared_to_1_minus_alpha = np.power(self.sumOfSquared, 2 * (1 - self.asymmetric_alpha))\n",
    "            self.sumOfSquared_to_alpha = np.power(self.sumOfSquared, 2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        # Apply weight after sumOfSquared has been computed but before the matrix is\n",
    "        # split in its inner data structures\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Row_weights has {} rows, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = np.array(row_weights, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csr')\n",
    "\n",
    "        self.user_to_item_row_ptr = dataMatrix.indptr\n",
    "        self.user_to_item_cols = dataMatrix.indices\n",
    "        self.user_to_item_data = np.array(dataMatrix.data, dtype=np.float64)\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csc')\n",
    "        self.item_to_user_rows = dataMatrix.indices\n",
    "        self.item_to_user_col_ptr = dataMatrix.indptr\n",
    "        self.item_to_user_data = np.array(dataMatrix.data, dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns,self.n_columns))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef useOnlyBooleanInteractions(self, dataMatrix):\n",
    "        \"\"\"\n",
    "        Set to 1 all data points\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef long index\n",
    "\n",
    "        for index in range(len(dataMatrix.data)):\n",
    "            dataMatrix.data[index] = 1\n",
    "\n",
    "        return dataMatrix\n",
    "\n",
    "\n",
    "\n",
    "    cdef applyPearsonCorrelation(self, dataMatrix):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef double[:] sumPerCol\n",
    "        cdef int[:] interactionsPerCol\n",
    "        cdef long colIndex, innerIndex, start_pos, end_pos\n",
    "        cdef double colAverage\n",
    "\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        sumPerCol = np.array(dataMatrix.sum(axis=0), dtype=np.float64).ravel()\n",
    "        interactionsPerCol = np.diff(dataMatrix.indptr)\n",
    "\n",
    "\n",
    "        #Remove for every row the corresponding average\n",
    "        for colIndex in range(self.n_columns):\n",
    "\n",
    "            if interactionsPerCol[colIndex]>0:\n",
    "\n",
    "                colAverage = sumPerCol[colIndex] / interactionsPerCol[colIndex]\n",
    "\n",
    "                start_pos = dataMatrix.indptr[colIndex]\n",
    "                end_pos = dataMatrix.indptr[colIndex+1]\n",
    "\n",
    "                innerIndex = start_pos\n",
    "\n",
    "                while innerIndex < end_pos:\n",
    "\n",
    "                    dataMatrix.data[innerIndex] -= colAverage\n",
    "                    innerIndex+=1\n",
    "\n",
    "\n",
    "        return dataMatrix\n",
    "\n",
    "\n",
    "\n",
    "    cdef applyAdjustedCosine(self, dataMatrix):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef double[:] sumPerRow\n",
    "        cdef int[:] interactionsPerRow\n",
    "        cdef long rowIndex, innerIndex, start_pos, end_pos\n",
    "        cdef double rowAverage\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csr')\n",
    "\n",
    "        sumPerRow = np.array(dataMatrix.sum(axis=1), dtype=np.float64).ravel()\n",
    "        interactionsPerRow = np.diff(dataMatrix.indptr)\n",
    "\n",
    "\n",
    "        #Remove for every row the corresponding average\n",
    "        for rowIndex in range(self.n_rows):\n",
    "\n",
    "            if interactionsPerRow[rowIndex]>0:\n",
    "\n",
    "                rowAverage = sumPerRow[rowIndex] / interactionsPerRow[rowIndex]\n",
    "\n",
    "                start_pos = dataMatrix.indptr[rowIndex]\n",
    "                end_pos = dataMatrix.indptr[rowIndex+1]\n",
    "\n",
    "                innerIndex = start_pos\n",
    "\n",
    "                while innerIndex < end_pos:\n",
    "\n",
    "                    dataMatrix.data[innerIndex] -= rowAverage\n",
    "                    innerIndex+=1\n",
    "\n",
    "\n",
    "        return dataMatrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "\n",
    "\n",
    "    #\n",
    "    # cdef data_pointer_s getUsersThatRatedItem_pointer(self, long item_id):\n",
    "    #\n",
    "    #     cdef data_pointer_s pointer = data_pointer_s()\n",
    "    #\n",
    "    #     pointer.start_position = self.item_to_user_col_ptr[item_id]\n",
    "    #     pointer.num_elements = self.item_to_user_col_ptr[item_id+1] - pointer.start_position\n",
    "    #\n",
    "    #     return pointer\n",
    "    #\n",
    "    # cdef data_pointer_s getItemsRatedByUser_pointer(self, long user_id):\n",
    "    #\n",
    "    #\n",
    "    #     cdef data_pointer_s pointer = data_pointer_s()\n",
    "    #\n",
    "    #     pointer.start_position = self.user_to_item_row_ptr[user_id]\n",
    "    #     pointer.num_elements = self.user_to_item_row_ptr[user_id+1] - pointer.start_position\n",
    "    #\n",
    "    #     return pointer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. The more\n",
    "        common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity for all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly the\n",
    "        information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_columns)\n",
    "        #cdef array[double] template_zero = array('d')\n",
    "        #cdef array[double] result = clone(template_zero, self.n_columns, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second, row_weight\n",
    "\n",
    "        # Clean previous item\n",
    "        for item_index in range(self.this_item_weights_counter):\n",
    "            item_id = self.this_item_weights_id[item_index]\n",
    "            self.this_item_weights_mask[item_id] = False\n",
    "            self.this_item_weights[item_id] = 0.0\n",
    "\n",
    "        self.this_item_weights_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                row_weight = self.row_weights[user_id]\n",
    "            else:\n",
    "                row_weight = 1.0\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    self.this_item_weights[item_id_second] += rating_item_input*rating_item_second*row_weight\n",
    "\n",
    "\n",
    "                    # Update global data structure\n",
    "                    if not self.this_item_weights_mask[item_id_second]:\n",
    "\n",
    "                        self.this_item_weights_mask[item_id_second] = True\n",
    "                        self.this_item_weights_id[self.this_item_weights_counter] = item_id_second\n",
    "                        self.this_item_weights_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int print_block_size = 500\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex, item_id, local_topK\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "        #cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_columns*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "        cdef int start_col_local = 0, end_col_local = self.n_columns\n",
    "\n",
    "        cdef array[double] template_zero = array('d')\n",
    "\n",
    "\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        last_print_time = start_time\n",
    "\n",
    "        itemIndex = start_col_local\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        while itemIndex < end_col_local:\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % print_block_size==0 or processedItems==end_col_local:\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Set block size to the number of items necessary in order to print every 30 seconds\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print_block_size = int(itemPerSec*30)\n",
    "\n",
    "                if current_time - last_print_time > 30  or processedItems==end_col_local:\n",
    "\n",
    "                    print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                        processedItems, processedItems*1.0/(end_col_local-start_col_local)*100, itemPerSec, (time.time()-start_time) / 60))\n",
    "\n",
    "                    last_print_time = current_time\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "\n",
    "            # Computed similarities go in self.this_item_weights\n",
    "            self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "\n",
    "            # Apply normalization and shrinkage, ensure denominator != 0\n",
    "            if self.normalize:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared_to_alpha[itemIndex] * self.sumOfSquared_to_1_minus_alpha[innerItemIndex]\\\n",
    "                                                             + self.shrink + 1e-6\n",
    "\n",
    "                    else:\n",
    "                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared[itemIndex] * self.sumOfSquared[innerItemIndex]\\\n",
    "                                                             + self.shrink + 1e-6\n",
    "\n",
    "            # Apply the specific denominator for Tanimoto\n",
    "            elif self.tanimoto_coefficient:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.this_item_weights[innerItemIndex] /= self.sumOfSquared[itemIndex] + self.sumOfSquared[innerItemIndex] -\\\n",
    "                                                         self.this_item_weights[innerItemIndex] + self.shrink + 1e-6\n",
    "\n",
    "            elif self.dice_coefficient:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.this_item_weights[innerItemIndex] /= self.sumOfSquared[itemIndex] + self.sumOfSquared[innerItemIndex] +\\\n",
    "                                                         self.shrink + 1e-6\n",
    "\n",
    "            elif self.tversky_coefficient:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.this_item_weights[innerItemIndex] /= self.this_item_weights[innerItemIndex] + \\\n",
    "                                                              (self.sumOfSquared[itemIndex]-self.this_item_weights[innerItemIndex])*self.tversky_alpha + \\\n",
    "                                                              (self.sumOfSquared[innerItemIndex]-self.this_item_weights[innerItemIndex])*self.tversky_beta +\\\n",
    "                                                              self.shrink + 1e-6\n",
    "\n",
    "            elif self.shrink != 0:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.this_item_weights[innerItemIndex] /= self.shrink\n",
    "\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = self.this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "                #\n",
    "\n",
    "\n",
    "\n",
    "                #this_item_weights_np = clone(template_zero, self.this_item_weights_counter, zero=False)\n",
    "                this_item_weights_np = np.zeros(self.n_columns, dtype=np.float64)\n",
    "\n",
    "                # Add weights in the same ordering as the self.this_item_weights_id data structure\n",
    "                for innerItemIndex in range(self.this_item_weights_counter):\n",
    "                    item_id = self.this_item_weights_id[innerItemIndex]\n",
    "                    this_item_weights_np[innerItemIndex] = - self.this_item_weights[item_id]\n",
    "\n",
    "\n",
    "                local_topK = min([self.TopK, self.this_item_weights_counter])\n",
    "\n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, local_topK-1)[0:local_topK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix, do not add zeros\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    item_id = self.this_item_weights_id[topKItemIndex]\n",
    "\n",
    "                    if self.this_item_weights[item_id] != 0.0:\n",
    "\n",
    "                        values[sparse_data_pointer] = self.this_item_weights[item_id]\n",
    "                        rows[sparse_data_pointer] = item_id\n",
    "                        cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                        sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "            itemIndex += 1\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_columns, self.n_columns),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cosine_common(X):\n",
    "    \"\"\"\n",
    "    Function that pairwise cosine similarity of the columns in X.\n",
    "    It takes only the values in common between each pair of columns\n",
    "    :param X: instance of scipy.sparse.csc_matrix\n",
    "    :return:\n",
    "        the result of co_prodsum\n",
    "        the number of co_rated elements for every column pair\n",
    "    \"\"\"\n",
    "\n",
    "    X = check_matrix(X, 'csc')\n",
    "\n",
    "    # use Cython MemoryViews for fast access to the sparse structure of X\n",
    "    cdef int [:] indices = X.indices\n",
    "    cdef int [:] indptr = X.indptr\n",
    "    cdef float [:] data = X.data\n",
    "\n",
    "    # initialize the result variables\n",
    "    cdef int n_cols = X.shape[1]\n",
    "    cdef np.ndarray[np.float32_t, ndim=2] result = np.zeros([n_cols, n_cols], dtype=np.float32)\n",
    "    cdef np.ndarray[np.int32_t, ndim=2] common = np.zeros([n_cols, n_cols], dtype=np.int32)\n",
    "\n",
    "    # let's declare all the variables that we'll use in the loop here\n",
    "    # NOTE: declaring the type of your variables makes your Cython code run MUCH faster\n",
    "    # NOTE: Cython allows cdef's only in the main scope\n",
    "    # cdef's in nested codes will result in compilation errors\n",
    "    cdef int current_col, second_col, n_i, n_j, ii, jj, n_common\n",
    "    cdef float ii_sum, jj_sum, ij_sum, x_i, x_j\n",
    "\n",
    "    for current_col in range(n_cols):\n",
    "        n_i = indptr[current_col+1] - indptr[current_col]\n",
    "        # the correlation matrix is symmetric,\n",
    "        # let's compute only the values for the upper-right triangle\n",
    "        for second_col in range(current_col+1, n_cols):\n",
    "            n_j = indptr[second_col+1] - indptr[second_col]\n",
    "\n",
    "            ij_sum, ii_sum, jj_sum = 0.0, 0.0, 0.0\n",
    "            ii, jj = 0, 0\n",
    "            n_common = 0\n",
    "\n",
    "            # here we exploit the fact that the two subvectors in indices are sorted\n",
    "            # to compute the dot product of the rows in common between i and j in linear time.\n",
    "            # (indices[indptr[i]:indptr[i]+n_i] and indices[indptr[j]:indptr[j]+n_j]\n",
    "            # contain the row indices of the non-zero items in columns i and j)\n",
    "            while ii < n_i and jj < n_j:\n",
    "                if indices[indptr[current_col] + ii] < indices[indptr[second_col] + jj]:\n",
    "                    ii += 1\n",
    "                elif indices[indptr[current_col] + ii] > indices[indptr[second_col] + jj]:\n",
    "                    jj += 1\n",
    "                else:\n",
    "                    x_i = data[indptr[current_col] + ii]\n",
    "                    x_j = data[indptr[second_col] + jj]\n",
    "                    ij_sum += x_i * x_j\n",
    "                    ii_sum += x_i ** 2\n",
    "                    jj_sum += x_j ** 2\n",
    "                    ii += 1\n",
    "                    jj += 1\n",
    "                    n_common += 1\n",
    "\n",
    "            if n_common > 0:\n",
    "                result[current_col, second_col] = ij_sum / np.sqrt(ii_sum * jj_sum)\n",
    "                result[second_col, current_col] = result[current_col, second_col]\n",
    "                common[current_col, second_col] = n_common\n",
    "                common[second_col, current_col] = n_common\n",
    "\n",
    "    return result, common\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#########################\n",
    "#########################       ARGSORT\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from libc.stdlib cimport malloc, free#, qsort\n",
    "\n",
    "# Declaring QSORT as \"gil safe\", appending \"nogil\" at the end of the declaration\n",
    "# Otherwise I will not be able to pass the comparator function pointer\n",
    "# https://stackoverflow.com/questions/8353076/how-do-i-pass-a-pointer-to-a-c-function-in-cython\n",
    "cdef extern from \"stdlib.h\":\n",
    "    ctypedef void const_void \"const void\"\n",
    "    void qsort(void *base, int nmemb, int size,\n",
    "            int(*compar)(const_void *, const_void *)) nogil\n",
    "\n",
    "\n",
    "\n",
    "# Node struct\n",
    "ctypedef struct matrix_element_s:\n",
    "    long coordinate\n",
    "    double data\n",
    "\n",
    "\n",
    "cdef int compare_struct_on_data(const void * a_input, const void * b_input):\n",
    "    \"\"\"\n",
    "    The function compares the data contained in the two struct passed.\n",
    "    If a.data > b.data returns >0  \n",
    "    If a.data < b.data returns <0      \n",
    "    \n",
    "    :return int: +1 or -1\n",
    "    \"\"\"\n",
    "\n",
    "    cdef matrix_element_s * a_casted = <matrix_element_s *> a_input\n",
    "    cdef matrix_element_s * b_casted = <matrix_element_s *> b_input\n",
    "\n",
    "    if (a_casted.data - b_casted.data) > 0.0:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cdef long[:] argsort(double[:] this_item_weights, int TopK):\n",
    "\n",
    "    #start_time = time.time()\n",
    "    cdef array[long] template_zero = array('l')\n",
    "    cdef array[long] result = clone(template_zero, TopK, zero=False)\n",
    "    #print(\"clone {} sec\".format(time.time()-start_time))\n",
    "\n",
    "    cdef matrix_element_s *matrix_element_array\n",
    "    cdef int index, num_elements\n",
    "\n",
    "    num_elements = len(this_item_weights)\n",
    "\n",
    "    # Allocate vector that will be used for sorting\n",
    "    matrix_element_array = < matrix_element_s *> malloc(num_elements * sizeof(matrix_element_s))\n",
    "\n",
    "    #start_time = time.time()\n",
    "\n",
    "    # Fill vector wit pointers to list elements\n",
    "    for index in range(num_elements):\n",
    "        matrix_element_array[index].coordinate = index\n",
    "        matrix_element_array[index].data = this_item_weights[index]\n",
    "\n",
    "    #print(\"Init {} sec\".format(time.time()-start_time))\n",
    "\n",
    "    #start_time = time.time()\n",
    "    # Sort array elements on their data field\n",
    "    qsort(matrix_element_array, num_elements, sizeof(matrix_element_s), compare_struct_on_data)\n",
    "    #print(\"qsort {} sec\".format(time.time()-start_time))\n",
    "\n",
    "    #start_time = time.time()\n",
    "    # Sort is from lower to higher, therefore the elements to be considered are from len-topK to len\n",
    "    for index in range(TopK):\n",
    "\n",
    "        result[index] = matrix_element_array[num_elements - index - 1].coordinate\n",
    "    #print(\"result {} sec\".format(time.time()-start_time))\n",
    "\n",
    "    free(matrix_element_array)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39737a8b32e53851edf1dbf534115cfca671f436"
   },
   "source": [
    "# DEF - Incremental Training Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "38137a0670a976507cbcba11b5a55fc948b89e24"
   },
   "outputs": [],
   "source": [
    "class Incremental_Training_Early_Stopping(object):\n",
    "    \"\"\"\n",
    "    This class provides a function which trains a model applying early stopping\n",
    "    The term \"incremental\" refers to the model that is updated at every epoch\n",
    "    The term \"best\" refers to the incremental model which corresponded to the best validation score\n",
    "    The object must implement the following methods:\n",
    "    __initialize_incremental_model(self)    : initializes the incremental model\n",
    "    _run_epoch(self, num_epoch)             : trains the model for one epoch (e.g. calling another object implementing the training cython, pyTorch...)\n",
    "    __update_incremental_model(self)        : updates the incremental model with the new one\n",
    "     __update_best_model(self)           : updates the best model with the current incremental one\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Incremental_Training_Early_Stopping, self).__init__()\n",
    "\n",
    "    def _initialize_incremental_model(self):\n",
    "        \"\"\"\n",
    "        This function should initialized the data structures required by the model you are going to train.\n",
    "        E.g. If the model uses a similarity matrix, here you should instantiate the global objects\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _run_epoch(self, num_epoch):\n",
    "        \"\"\"\n",
    "        This function should run a single epoch on the object you train. This may either involve calling a function to do an epoch\n",
    "        on a Cython object or a loop on the data points directly in python\n",
    "        :param num_epoch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _update_incremental_model(self):\n",
    "        \"\"\"\n",
    "        This function is executed before the evaluation of the current model\n",
    "        It should ensure the current object \"self\" can be passed to the evaluator object\n",
    "        E.G. if the epoch is done via Cython or PyTorch, this function should get the new parameter values from\n",
    "        the cython or pytorch objects into the self. pyhon object\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        \"\"\"\n",
    "        This function is called when the incremental model is found to have better validation score than the current best one\n",
    "        So the current best model should be replaced by the current incremental one.\n",
    "        Important, remember to clone the objects and NOT to create a pointer-reference, otherwise the best solution will be altered\n",
    "        by the next epoch\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # def _validation_incremental_model(self):\n",
    "    #     raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _train_with_early_stopping(self, epochs, validation_every_n, stop_on_validation,\n",
    "                                    validation_metric, lower_validatons_allowed, evaluator_object,\n",
    "                                    algorithm_name = \"Incremental_Training_Early_Stopping\"):\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.best_validation_metric = None\n",
    "        lower_validatons_count = 0\n",
    "        convergence = False\n",
    "\n",
    "        self._initialize_incremental_model()\n",
    "\n",
    "        self.epochs_best = 0\n",
    "\n",
    "        currentEpoch = 0\n",
    "\n",
    "        while currentEpoch < epochs and not convergence:\n",
    "\n",
    "            self._run_epoch(currentEpoch)\n",
    "\n",
    "            # Determine whether a validaton step is required\n",
    "            if evaluator_object is not None and (currentEpoch + 1) % validation_every_n == 0:\n",
    "\n",
    "                print(\"{}: Validation begins...\".format(algorithm_name))\n",
    "\n",
    "                self._update_incremental_model()\n",
    "\n",
    "                results_run, _ = evaluator_object.evaluateRecommender(self)\n",
    "                results_run = results_run[list(results_run.keys())[0]]\n",
    "\n",
    "                print(\"{}: {}\".format(algorithm_name, results_run))\n",
    "\n",
    "                # Update the D_best and V_best\n",
    "                # If validation is required, check whether result is better\n",
    "                if stop_on_validation:\n",
    "\n",
    "                    current_metric_value = results_run[validation_metric]\n",
    "\n",
    "                    if self.best_validation_metric is None or self.best_validation_metric < current_metric_value:\n",
    "\n",
    "                        self.best_validation_metric = current_metric_value\n",
    "\n",
    "                        self._update_best_model()\n",
    "\n",
    "                        self.epochs_best = currentEpoch +1\n",
    "                        lower_validatons_count = 0\n",
    "\n",
    "                    else:\n",
    "                        lower_validatons_count += 1\n",
    "\n",
    "                    if lower_validatons_count >= lower_validatons_allowed:\n",
    "                        convergence = True\n",
    "                        print(\"{}: Convergence reached! Terminating at epoch {}. Best value for '{}' at epoch {} is {:.4f}. Elapsed time {:.2f} min\".format(\n",
    "                            algorithm_name, currentEpoch+1, validation_metric, self.epochs_best, self.best_validation_metric, (time.time() - start_time) / 60))\n",
    "\n",
    "                else:\n",
    "                    self.epochs_best = currentEpoch\n",
    "\n",
    "            # If no validation required, always keep the latest\n",
    "            if not stop_on_validation:\n",
    "                self._update_best_model()\n",
    "\n",
    "            print(\"{}: Epoch {} of {}. Elapsed time {:.2f} min\".format(\n",
    "                algorithm_name, currentEpoch+1, epochs, (time.time() - start_time) / 60))\n",
    "\n",
    "            currentEpoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2388cd6a6ef0f1cb55bec9c792d040ba4b9ba885"
   },
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "fb695bce971d36f316b237508c277cd855588330"
   },
   "outputs": [],
   "source": [
    "def train_test_holdout(URM_all, train_perc = 0.8):\n",
    "\n",
    "    numInteractions = URM_all.nnz\n",
    "    URM_all = URM_all.tocoo()\n",
    "\n",
    "\n",
    "    train_mask = np.random.choice([True,False], numInteractions, p=[train_perc, 1-train_perc])\n",
    "\n",
    "\n",
    "    URM_train = sps.coo_matrix((URM_all.data[train_mask], (URM_all.row[train_mask], URM_all.col[train_mask])))\n",
    "    URM_train = URM_train.tocsr()\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)\n",
    "\n",
    "    URM_test = sps.coo_matrix((URM_all.data[test_mask], (URM_all.row[test_mask], URM_all.col[test_mask])))\n",
    "    URM_test = URM_test.tocsr()\n",
    "\n",
    "    return URM_train, URM_test\n",
    "\n",
    "\n",
    "def train_test_holdout_adjusted(URM_all, train_perc = 0.8):\n",
    "\n",
    "    print('start_split')\n",
    "\n",
    "    URM_all = URM_all.tocoo()\n",
    "\n",
    "    temp_col_num = 0\n",
    "    train_mask = np.array([]).astype(bool)\n",
    "    prev_row = URM_all.row[0]\n",
    "\n",
    "    for k in range(len(URM_all.row)):\n",
    "\n",
    "        if URM_all.row[k] == prev_row:\n",
    "            temp_col_num += 1\n",
    "        else:\n",
    "            if temp_col_num >= 10:\n",
    "                # temp_mask = np.random.choice([True, False], temp_col_num, p=[train_perc, 1 - train_perc])\n",
    "                temp_mask = np.append(np.repeat(True, temp_col_num-10), np.repeat(False, 10))\n",
    "                np.random.shuffle(temp_mask)\n",
    "            else:\n",
    "                temp_mask = np.repeat(True, temp_col_num)\n",
    "\n",
    "            train_mask = np.append(train_mask, temp_mask)\n",
    "            temp_col_num = 1\n",
    "\n",
    "        if k == len(URM_all.row)-1:\n",
    "            temp_mask = np.append(np.repeat(True, temp_col_num-10), np.repeat(False, 10))\n",
    "            #temp_mask = np.random.choice([True, False], temp_col_num, p=[train_perc, 1 - train_perc])\n",
    "            train_mask = np.append(train_mask, temp_mask)\n",
    "\n",
    "        prev_row = URM_all.row[k]\n",
    "\n",
    "    URM_train = sps.coo_matrix((URM_all.data[train_mask], (URM_all.row[train_mask], URM_all.col[train_mask])), shape=URM_all.shape)\n",
    "    URM_train = URM_train.tocsr()\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)\n",
    "\n",
    "    URM_test = sps.coo_matrix((URM_all.data[test_mask], (URM_all.row[test_mask], URM_all.col[test_mask])))\n",
    "    URM_test = URM_all.tocsr()\n",
    "\n",
    "    return URM_train, URM_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfe517ab4ecc4bc2509b700bba8eadb62d0ecda6"
   },
   "source": [
    "# DEF - BPR Sampling - Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "da888c13b67be76604cd8c2df7bfd4908fa28ffe"
   },
   "outputs": [],
   "source": [
    "class BPR_Sampling(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BPR_Sampling, self).__init__()\n",
    "\n",
    "\n",
    "    def sampleUser(self):\n",
    "        \"\"\"\n",
    "        Sample a user that has viewed at least one and not all items\n",
    "        :return: user_id\n",
    "        \"\"\"\n",
    "        while (True):\n",
    "\n",
    "            user_id = np.random.randint(0, self.n_users)\n",
    "            numSeenItems = self.URM_train[user_id].nnz\n",
    "\n",
    "            if (numSeenItems > 0 and numSeenItems < self.n_items):\n",
    "                return user_id\n",
    "\n",
    "\n",
    "    def sampleItemPair(self, user_id):\n",
    "        \"\"\"\n",
    "        Returns for the given user a random seen item and a random not seen item\n",
    "        :param user_id:\n",
    "        :return: pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        userSeenItems = self.URM_train[user_id].indices\n",
    "\n",
    "        pos_item_id = userSeenItems[np.random.randint(0, len(userSeenItems))]\n",
    "\n",
    "        while (True):\n",
    "\n",
    "            neg_item_id = np.random.randint(0, self.n_items)\n",
    "\n",
    "            if (neg_item_id not in userSeenItems):\n",
    "                return pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "    def sampleTriple(self):\n",
    "        \"\"\"\n",
    "        Randomly samples a user and then samples randomly a seen and not seen item\n",
    "        :return: user_id, pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        user_id = self.sampleUser()\n",
    "        pos_item_id, neg_item_id = self.sampleItemPair(user_id)\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "    def initializeFastSampling(self, positive_threshold=3):\n",
    "        print(\"Initializing fast sampling\")\n",
    "\n",
    "        self.eligibleUsers = []\n",
    "        self.userSeenItems = dict()\n",
    "\n",
    "        # Select only positive interactions\n",
    "        URM_train_positive = self.URM_train.multiply(self.URM_train>positive_threshold)\n",
    "\n",
    "        for user_id in range(self.n_users):\n",
    "\n",
    "            if (URM_train_positive[user_id].nnz > 0):\n",
    "                self.eligibleUsers.append(user_id)\n",
    "                self.userSeenItems[user_id] = URM_train_positive[user_id].indices\n",
    "\n",
    "        self.eligibleUsers = np.array(self.eligibleUsers)\n",
    "\n",
    "\n",
    "    def sampleBatch(self):\n",
    "        user_id_list = np.random.choice(self.eligibleUsers, size=(self.batch_size))\n",
    "        pos_item_id_list = [None]*self.batch_size\n",
    "        neg_item_id_list = [None]*self.batch_size\n",
    "\n",
    "        for sample_index in range(self.batch_size):\n",
    "            user_id = user_id_list[sample_index]\n",
    "\n",
    "            pos_item_id_list[sample_index] = np.random.choice(self.userSeenItems[user_id])\n",
    "\n",
    "            negItemSelected = False\n",
    "\n",
    "            # It's faster to just try again then to build a mapping of the non-seen items\n",
    "            # for every user\n",
    "            while (not negItemSelected):\n",
    "                neg_item_id = np.random.randint(0, self.n_items)\n",
    "\n",
    "                if (neg_item_id not in self.userSeenItems[user_id]):\n",
    "                    negItemSelected = True\n",
    "                    neg_item_id_list[sample_index] = neg_item_id\n",
    "\n",
    "        return user_id_list, pos_item_id_list, neg_item_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d7eabcfa0dade3cd317779f6fde276fa875ea89"
   },
   "source": [
    "# DEF - Cython Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "f110a6c51e4aa939982d6b20ede361135c8c150d"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import time, sys\n",
    "import subprocess\n",
    "\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "from libc.math cimport exp, sqrt\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            idx_sorted = np.argsort(column_data)  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[top_k_idx])\n",
    "            rows_indices.extend(column_row_index[top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n",
    "cdef struct BPR_sample:\n",
    "    long user\n",
    "    long pos_item\n",
    "    long neg_item\n",
    "    long seen_items_start_pos\n",
    "    long seen_items_end_pos\n",
    "\n",
    "\n",
    "\n",
    "cdef class SLIM_BPR_Cython_Epoch:\n",
    "\n",
    "    cdef int n_users\n",
    "    cdef int n_items\n",
    "    cdef int numPositiveIteractions\n",
    "    cdef int topK\n",
    "    cdef int symmetric, train_with_sparse_weights, final_model_sparse_weights\n",
    "\n",
    "    cdef double learning_rate, li_reg, lj_reg\n",
    "\n",
    "    cdef int batch_size\n",
    "\n",
    "\n",
    "    cdef int[:] URM_mask_indices, URM_mask_indptr\n",
    "\n",
    "    cdef Sparse_Matrix_Tree_CSR S_sparse\n",
    "    cdef Triangular_Matrix S_symmetric\n",
    "    cdef double[:,:] S_dense\n",
    "\n",
    "\n",
    "    # Adaptive gradient\n",
    "\n",
    "    cdef int useAdaGrad, useRmsprop, useAdam\n",
    "\n",
    "    cdef double [:] sgd_cache_I\n",
    "    cdef double gamma\n",
    "\n",
    "    cdef double [:] sgd_cache_I_momentum_1, sgd_cache_I_momentum_2\n",
    "    cdef double beta_1, beta_2, beta_1_power_t, beta_2_power_t\n",
    "    cdef double momentum_1, momentum_2\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, URM_mask,\n",
    "                 train_with_sparse_weights = False,\n",
    "                 final_model_sparse_weights = True,\n",
    "                 learning_rate = 0.01, li_reg = 0.0, lj_reg = 0.0,\n",
    "                 batch_size = 1, topK = 150, symmetric = True,\n",
    "                 sgd_mode='adam', gamma=0.995, beta_1=0.9, beta_2=0.999):\n",
    "\n",
    "        super(SLIM_BPR_Cython_Epoch, self).__init__()\n",
    "\n",
    "        URM_mask = check_matrix(URM_mask, 'csr')\n",
    "\n",
    "        self.numPositiveIteractions = int(URM_mask.nnz * 1)\n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "        self.topK = topK\n",
    "        self.learning_rate = learning_rate\n",
    "        self.li_reg = li_reg\n",
    "        self.lj_reg = lj_reg\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "        if train_with_sparse_weights:\n",
    "            symmetric = False\n",
    "\n",
    "        self.train_with_sparse_weights = train_with_sparse_weights\n",
    "        self.final_model_sparse_weights = final_model_sparse_weights\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        self.URM_mask_indices = np.array(URM_mask.indices, dtype=np.int32)\n",
    "        self.URM_mask_indptr = np.array(URM_mask.indptr, dtype=np.int32)\n",
    "\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.S_sparse = Sparse_Matrix_Tree_CSR(self.n_items, self.n_items)\n",
    "\n",
    "        elif self.symmetric:\n",
    "            self.S_symmetric = Triangular_Matrix(self.n_items, isSymmetric = True)\n",
    "        else:\n",
    "            self.S_dense = np.zeros((self.n_items, self.n_items), dtype=np.float64)\n",
    "\n",
    "\n",
    "        self.useAdaGrad = False\n",
    "        self.useRmsprop = False\n",
    "        self.useAdam = False\n",
    "\n",
    "\n",
    "        if sgd_mode=='adagrad':\n",
    "            self.useAdaGrad = True\n",
    "            self.sgd_cache_I = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "        elif sgd_mode=='rmsprop':\n",
    "            self.useRmsprop = True\n",
    "            self.sgd_cache_I = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "            # Gamma default value suggested by Hinton\n",
    "            # self.gamma = 0.9\n",
    "            self.gamma = gamma\n",
    "\n",
    "        elif sgd_mode=='adam':\n",
    "            self.useAdam = True\n",
    "            self.sgd_cache_I_momentum_1 = np.zeros((self.n_items), dtype=np.float64)\n",
    "            self.sgd_cache_I_momentum_2 = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "            # Default value suggested by the original paper\n",
    "            # beta_1=0.9, beta_2=0.999\n",
    "            self.beta_1 = beta_1\n",
    "            self.beta_2 = beta_2\n",
    "            self.beta_1_power_t = beta_1\n",
    "            self.beta_2_power_t = beta_2\n",
    "\n",
    "        elif sgd_mode=='sgd':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"SGD_mode not valid. Acceptable values are: 'sgd', 'adagrad', 'rmsprop', 'adam'. Provided value was '{}'\".format(\n",
    "                    sgd_mode))\n",
    "\n",
    "\n",
    "    def __dealloc__(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.S_sparse.dealloc()\n",
    "\n",
    "        elif self.symmetric:\n",
    "            self.S_symmetric.dealloc()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def epochIteration_Cython(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long totalNumberOfBatch = int(self.numPositiveIteractions / self.batch_size) + 1\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long start_time_batch = time.time()\n",
    "\n",
    "        cdef BPR_sample sample\n",
    "        cdef long i, j\n",
    "        cdef long index, seenItem, numCurrentBatch, itemId\n",
    "        cdef double x_uij, gradient, loss = 0.0\n",
    "        cdef double gradient_update\n",
    "\n",
    "        cdef int numSeenItems\n",
    "        cdef int printStep\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            printStep = 500000\n",
    "        else:\n",
    "            printStep = 5000000\n",
    "\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for numCurrentBatch in range(totalNumberOfBatch):\n",
    "\n",
    "            sample = self.sampleBPR_Cython()\n",
    "\n",
    "            i = sample.pos_item\n",
    "            j = sample.neg_item\n",
    "\n",
    "            x_uij = 0.0\n",
    "\n",
    "            # The difference is computed on the user_seen items\n",
    "\n",
    "            index = 0\n",
    "            while index <  sample.seen_items_end_pos - sample.seen_items_start_pos:\n",
    "\n",
    "                seenItem = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "                index +=1\n",
    "\n",
    "                if self.train_with_sparse_weights:\n",
    "                   x_uij += self.S_sparse.get_value(i, seenItem) - self.S_sparse.get_value(j, seenItem)\n",
    "\n",
    "                elif self.symmetric:\n",
    "                    x_uij += self.S_symmetric.get_value(i, seenItem) - self.S_symmetric.get_value(j, seenItem)\n",
    "\n",
    "                else:\n",
    "                    x_uij += self.S_dense[i, seenItem] - self.S_dense[j, seenItem]\n",
    "\n",
    "\n",
    "            gradient = 1 / (1 + exp(x_uij))\n",
    "            loss += x_uij**2\n",
    "\n",
    "\n",
    "            # if self.useAdaGrad:\n",
    "            #     cacheUpdate = gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            # elif self.useRmsprop:\n",
    "            #     cacheUpdate = sgd_cache[i] * gamma + (1 - gamma) * gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] = cacheUpdate\n",
    "            #     sgd_cache[j] = cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            #\n",
    "            # #######################################\n",
    "\n",
    "\n",
    "            if self.useAdaGrad:\n",
    "                self.sgd_cache_I[i] += gradient ** 2\n",
    "                self.sgd_cache_I[j] += gradient ** 2\n",
    "\n",
    "                gradient_update = gradient / (sqrt(self.sgd_cache_I[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            elif self.useRmsprop:\n",
    "                self.sgd_cache_I[i] = self.sgd_cache_I[i] * self.gamma + (1 - self.gamma) * gradient ** 2\n",
    "                self.sgd_cache_I[j] = self.sgd_cache_I[j] * self.gamma + (1 - self.gamma) * gradient ** 2\n",
    "\n",
    "                gradient_update = gradient / (sqrt(self.sgd_cache_I[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            elif self.useAdam:\n",
    "\n",
    "                self.sgd_cache_I_momentum_1[i] = \\\n",
    "                    self.sgd_cache_I_momentum_1[i] * self.beta_1 + (1 - self.beta_1) * gradient\n",
    "\n",
    "                self.sgd_cache_I_momentum_2[i] = \\\n",
    "                    self.sgd_cache_I_momentum_2[i] * self.beta_2 + (1 - self.beta_2) * gradient**2\n",
    "\n",
    "\n",
    "                self.momentum_1 = self.sgd_cache_I_momentum_1[i]/ (1 - self.beta_1_power_t)\n",
    "                self.momentum_2 = self.sgd_cache_I_momentum_2[i]/ (1 - self.beta_2_power_t)\n",
    "\n",
    "                gradient_update = self.momentum_1/ (sqrt(self.momentum_2) + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                self.sgd_cache_I_momentum_1[j] = \\\n",
    "                    self.sgd_cache_I_momentum_1[j] * self.beta_1 + (1 - self.beta_1) * gradient\n",
    "\n",
    "                self.sgd_cache_I_momentum_2[j] = \\\n",
    "                    self.sgd_cache_I_momentum_2[j] * self.beta_2 + (1 - self.beta_2) * gradient**2\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                gradient_update = gradient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            index = 0\n",
    "            while index < sample.seen_items_end_pos - sample.seen_items_start_pos:\n",
    "\n",
    "                seenItem = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "                index +=1\n",
    "\n",
    "                if self.train_with_sparse_weights:\n",
    "                    # Since the sparse matrix is slower compared to the others\n",
    "                    # If no reg is required, avoid accessing it\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        if self.li_reg!= 0.0:\n",
    "                            self.S_sparse.add_value(i, seenItem, self.learning_rate * (gradient_update - self.li_reg * self.S_sparse.get_value(i, seenItem)))\n",
    "                        else:\n",
    "                            self.S_sparse.add_value(i, seenItem, self.learning_rate * gradient_update)\n",
    "\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        if self.lj_reg!= 0.0:\n",
    "                            self.S_sparse.add_value(j, seenItem, -self.learning_rate * (gradient_update - self.lj_reg * self.S_sparse.get_value(j, seenItem)))\n",
    "                        else:\n",
    "                            self.S_sparse.add_value(j, seenItem, -self.learning_rate * gradient_update)\n",
    "\n",
    "\n",
    "                elif self.symmetric:\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        self.S_symmetric.add_value(i, seenItem, self.learning_rate * (gradient_update - self.li_reg * self.S_symmetric.get_value(i, seenItem)))\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        self.S_symmetric.add_value(j, seenItem, -self.learning_rate * (gradient_update - self.lj_reg * self.S_symmetric.get_value(j, seenItem)))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        self.S_dense[i, seenItem] += self.learning_rate * (gradient_update - self.li_reg * self.S_dense[i, seenItem])\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        self.S_dense[j, seenItem] -= self.learning_rate * (gradient_update - self.lj_reg * self.S_dense[j, seenItem])\n",
    "\n",
    "\n",
    "\n",
    "            # Exponentiation of beta at the end of each sample\n",
    "            if self.useAdam:\n",
    "\n",
    "                self.beta_1_power_t *= self.beta_1\n",
    "                self.beta_2_power_t *= self.beta_2\n",
    "\n",
    "\n",
    "\n",
    "            # If I have reached at least 20% of the total number of batches or samples\n",
    "            # This allows to limit the memory occupancy of the sparse matrix\n",
    "            if self.train_with_sparse_weights and numCurrentBatch % (totalNumberOfBatch/5) == 0 and numCurrentBatch!=0:\n",
    "                self.S_sparse.rebalance_tree(TopK=self.topK)\n",
    "\n",
    "\n",
    "            if((numCurrentBatch%printStep==0 and not numCurrentBatch==0) or numCurrentBatch==totalNumberOfBatch-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. BPR loss is {:.2E}. Sample per second: {:.0f}\".format(\n",
    "                    numCurrentBatch*self.batch_size,\n",
    "                    100.0* float(numCurrentBatch*self.batch_size)/self.numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    loss/(numCurrentBatch*self.batch_size + 1),\n",
    "                    float(numCurrentBatch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_S(self):\n",
    "\n",
    "        # FIll diagonal with zeros\n",
    "        cdef int index = 0\n",
    "\n",
    "        while index < self.n_items:\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                self.S_sparse.add_value(index, index, -self.S_sparse.get_value(index, index))\n",
    "\n",
    "            elif self.symmetric:\n",
    "                self.S_symmetric.add_value(index, index, -self.S_symmetric.get_value(index, index))\n",
    "\n",
    "            else:\n",
    "                self.S_dense[index, index] = 0.0\n",
    "\n",
    "            index+=1\n",
    "\n",
    "\n",
    "\n",
    "        if self.topK == False:\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                return self.S_sparse.get_scipy_csr(TopK = False)\n",
    "\n",
    "            elif self.symmetric:\n",
    "                return self.S_symmetric.get_scipy_csr(TopK = False)\n",
    "\n",
    "            else:\n",
    "\n",
    "                if self.final_model_sparse_weights:\n",
    "                    return similarityMatrixTopK(np.array(self.S_dense.T), k=self.topK, forceSparseOutput=True, inplace=True).T\n",
    "                else:\n",
    "                    return np.array(self.S_dense)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                return self.S_sparse.get_scipy_csr(TopK=self.topK)\n",
    "\n",
    "            elif self.symmetric:\n",
    "                return self.S_symmetric.get_scipy_csr(TopK=self.topK)\n",
    "\n",
    "            else:\n",
    "                if self.final_model_sparse_weights:\n",
    "                    return similarityMatrixTopK(np.array(self.S_dense.T), k=self.topK, forceSparseOutput=True, inplace=True).T\n",
    "                else:\n",
    "                    return np.array(self.S_dense)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef BPR_sample sampleBPR_Cython(self):\n",
    "\n",
    "        cdef BPR_sample sample = BPR_sample(-1,-1,-1,-1,-1)\n",
    "\n",
    "        cdef long index\n",
    "\n",
    "        cdef int negItemSelected, numSeenItems = 0\n",
    "\n",
    "\n",
    "        # Skip users with no interactions or with no negative items\n",
    "        while numSeenItems == 0 or numSeenItems == self.n_items:\n",
    "\n",
    "            sample.user = rand() % self.n_users\n",
    "\n",
    "            sample.seen_items_start_pos = self.URM_mask_indptr[sample.user]\n",
    "            sample.seen_items_end_pos = self.URM_mask_indptr[sample.user + 1]\n",
    "\n",
    "            numSeenItems = sample.seen_items_end_pos - sample.seen_items_start_pos\n",
    "\n",
    "\n",
    "        index = rand() % numSeenItems\n",
    "\n",
    "        sample.pos_item = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        # for every user\n",
    "        while (not negItemSelected):\n",
    "\n",
    "            sample.neg_item = rand() % self.n_items\n",
    "\n",
    "            index = 0\n",
    "            while index < numSeenItems and self.URM_mask_indices[sample.seen_items_start_pos + index]!=sample.neg_item:\n",
    "                index+=1\n",
    "\n",
    "            if index == numSeenItems:\n",
    "                negItemSelected = True\n",
    "\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            SPARSE MATRIX\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "#from libc.stdlib cimport malloc, free#, qsort\n",
    "# PyMem malloc and free are slightly faster than plain C equivalents as they optimize OS calls\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "# Declaring QSORT as \"gil safe\", appending \"nogil\" at the end of the declaration\n",
    "# Otherwise I will not be able to pass the comparator function pointer\n",
    "# https://stackoverflow.com/questions/8353076/how-do-i-pass-a-pointer-to-a-c-function-in-cython\n",
    "cdef extern from \"stdlib.h\":\n",
    "    ctypedef void const_void \"const void\"\n",
    "    void qsort(void *base, int nmemb, int size,\n",
    "            int(*compar)(const_void *, const_void *)) nogil\n",
    "\n",
    "\n",
    "# Node struct\n",
    "ctypedef struct matrix_element_tree_s:\n",
    "    long column\n",
    "    double data\n",
    "    matrix_element_tree_s *higher\n",
    "    matrix_element_tree_s *lower\n",
    "\n",
    "ctypedef struct head_pointer_tree_s:\n",
    "    matrix_element_tree_s *head\n",
    "\n",
    "\n",
    "# Function to allocate a new node\n",
    "cdef matrix_element_tree_s * pointer_new_matrix_element_tree_s(long column, double data, matrix_element_tree_s *higher,  matrix_element_tree_s *lower):\n",
    "\n",
    "    cdef matrix_element_tree_s * new_element\n",
    "\n",
    "    new_element = < matrix_element_tree_s * > PyMem_Malloc(sizeof(matrix_element_tree_s))\n",
    "    new_element.column = column\n",
    "    new_element.data = data\n",
    "    new_element.higher = higher\n",
    "    new_element.lower = lower\n",
    "\n",
    "    return new_element\n",
    "\n",
    "\n",
    "# Functions to compare structs to be used in C qsort\n",
    "cdef int compare_struct_on_column(const void *a_input, const void *b_input):\n",
    "    \"\"\"\n",
    "    The function compares the column contained in the two struct passed.\n",
    "    If a.column > b.column returns >0  \n",
    "    If a.column < b.column returns <0      \n",
    "    \n",
    "    :return int: a.column - b.column\n",
    "    \"\"\"\n",
    "\n",
    "    cdef head_pointer_tree_s *a_casted = <head_pointer_tree_s *> a_input\n",
    "    cdef head_pointer_tree_s *b_casted = <head_pointer_tree_s *> b_input\n",
    "\n",
    "    return a_casted.head.column  - b_casted.head.column\n",
    "\n",
    "\n",
    "\n",
    "cdef int compare_struct_on_data(const void * a_input, const void * b_input):\n",
    "    \"\"\"\n",
    "    The function compares the data contained in the two struct passed.\n",
    "    If a.data > b.data returns >0  \n",
    "    If a.data < b.data returns <0      \n",
    "    \n",
    "    :return int: +1 or -1\n",
    "    \"\"\"\n",
    "\n",
    "    cdef head_pointer_tree_s * a_casted = <head_pointer_tree_s *> a_input\n",
    "    cdef head_pointer_tree_s * b_casted = <head_pointer_tree_s *> b_input\n",
    "\n",
    "    if (a_casted.head.data - b_casted.head.data) > 0.0:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "#################################       CLASS DECLARATION\n",
    "#################################\n",
    "\n",
    "cdef class Sparse_Matrix_Tree_CSR:\n",
    "\n",
    "    cdef long num_rows, num_cols\n",
    "\n",
    "    # Array containing the struct (object, not pointer) corresponding to the root of the tree\n",
    "    cdef head_pointer_tree_s* row_pointer\n",
    "\n",
    "\n",
    "    def __init__(self, long num_rows, long num_cols):\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "        self.row_pointer = < head_pointer_tree_s *> PyMem_Malloc(self.num_rows * sizeof(head_pointer_tree_s))\n",
    "\n",
    "        # Initialize all rows to empty\n",
    "        for index in range(self.num_rows):\n",
    "            self.row_pointer[index].head = NULL\n",
    "\n",
    "\n",
    "\n",
    "    def dealloc(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int numRow\n",
    "\n",
    "        # Free all rows memory\n",
    "        for numRow in range(self.num_rows):\n",
    "            self.subtree_free_memory(self.row_pointer[numRow].head)\n",
    "\n",
    "        PyMem_Free(self.row_pointer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double add_value(self, long row, long col, double value):\n",
    "        \"\"\"\n",
    "        The function adds a value to the specified cell. A new cell is created if necessary.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :param value: value to add\n",
    "        :return double: resulting cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        cdef matrix_element_tree_s* current_element, new_element, * old_element\n",
    "        cdef int stopSearch = False\n",
    "\n",
    "\n",
    "        # If the row is empty, create a new element\n",
    "        if self.row_pointer[row].head == NULL:\n",
    "\n",
    "            # row_pointer is a python object, so I need the object itself and not the address\n",
    "            self.row_pointer[row].head = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "\n",
    "        # If the row is not empty, look for the cell\n",
    "        # row_pointer contains the struct itself, but I just want its address\n",
    "        current_element = self.row_pointer[row].head\n",
    "\n",
    "        # Follow the tree structure\n",
    "        while not stopSearch:\n",
    "\n",
    "            if current_element.column < col and current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            elif current_element.column > col and current_element.lower != NULL:\n",
    "                current_element = current_element.lower\n",
    "\n",
    "            else:\n",
    "                stopSearch = True\n",
    "\n",
    "        # If the cell exist, update its value\n",
    "        if current_element.column == col:\n",
    "            current_element.data += value\n",
    "\n",
    "            return current_element.data\n",
    "\n",
    "\n",
    "        # The cell is not found, create new Higher element\n",
    "        elif current_element.column < col and current_element.higher == NULL:\n",
    "\n",
    "            current_element.higher = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "        # The cell is not found, create new Lower element\n",
    "        elif current_element.column > col and current_element.lower == NULL:\n",
    "\n",
    "            current_element.lower = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "        else:\n",
    "            assert False, 'ERROR - Current insert operation is not implemented'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double get_value(self, long row, long col):\n",
    "        \"\"\"\n",
    "        The function returns the value of the specified cell.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :return double: cell value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\n",
    "                \"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                    self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "\n",
    "        cdef matrix_element_tree_s* current_element\n",
    "        cdef int stopSearch = False\n",
    "\n",
    "        # If the row is empty, return default\n",
    "        if self.row_pointer[row].head == NULL:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "        # If the row is not empty, look for the cell\n",
    "        # row_pointer contains the struct itself, but I just want its address\n",
    "        current_element = self.row_pointer[row].head\n",
    "\n",
    "        # Follow the tree structure\n",
    "        while not stopSearch:\n",
    "\n",
    "            if current_element.column < col and current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            elif current_element.column > col and current_element.lower != NULL:\n",
    "                current_element = current_element.lower\n",
    "\n",
    "            else:\n",
    "                stopSearch = True\n",
    "\n",
    "\n",
    "        # If the cell exist, return its value\n",
    "        if current_element.column == col:\n",
    "            return current_element.data\n",
    "\n",
    "        # The cell is not found, return default\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef get_scipy_csr(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function returns the current sparse matrix as a scipy_csr object         \n",
    "   \n",
    "        :return double: scipy_csr object\n",
    "        \"\"\"\n",
    "        cdef int terminate\n",
    "        cdef long row\n",
    "\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        # Loop the rows\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            #Always set indptr\n",
    "            indptr.append(len(data))\n",
    "\n",
    "            # row contains data\n",
    "            if self.row_pointer[row].head != NULL:\n",
    "\n",
    "                # Flatten the data structure\n",
    "                self.row_pointer[row].head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"subtree_to_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                if TopK:\n",
    "                    self.row_pointer[row].head = self.topK_selection_from_list(self.row_pointer[row].head, TopK)\n",
    "                    #print(\"topK_selection_from_list {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "                # Flatten the tree data\n",
    "                subtree_column, subtree_data = self.from_linked_list_to_python_list(self.row_pointer[row].head)\n",
    "                data.extend(subtree_data)\n",
    "                indices.extend(subtree_column)\n",
    "\n",
    "                # Rebuild the tree\n",
    "                self.row_pointer[row].head = self.build_tree_from_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"build_tree_from_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "        #Set terminal indptr\n",
    "        indptr.append(len(data))\n",
    "\n",
    "        return sps.csr_matrix((data, indices, indptr), shape=(self.num_rows, self.num_cols))\n",
    "\n",
    "\n",
    "\n",
    "    cpdef rebalance_tree(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function builds a balanced binary tree from the current one, for all matrix rows\n",
    "        \n",
    "        :param TopK: either False or an integer number. Number of the highest elements to preserve\n",
    "        \"\"\"\n",
    "\n",
    "        cdef long row\n",
    "\n",
    "        #start_time = time.time()\n",
    "\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            if self.row_pointer[row].head != NULL:\n",
    "\n",
    "                # Flatten the data structure\n",
    "                self.row_pointer[row].head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"subtree_to_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                if TopK:\n",
    "                    self.row_pointer[row].head = self.topK_selection_from_list(self.row_pointer[row].head, TopK)\n",
    "                    #print(\"topK_selection_from_list {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                # Rebuild the tree\n",
    "                self.row_pointer[row].head = self.build_tree_from_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"build_tree_from_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s * subtree_to_list_flat(self, matrix_element_tree_s * root):\n",
    "        \"\"\"\n",
    "        The function flatten the structure of the subtree whose root is passed as a paramether    \n",
    "        The list is bidirectional and ordered with respect to the column\n",
    "        The column ordering follows from the insertion policy\n",
    "        \n",
    "        :param root: tree root\n",
    "        :return list, list: data and corresponding column. Empty list if root is None\n",
    "        \"\"\"\n",
    "\n",
    "        if root == NULL:\n",
    "            return NULL\n",
    "\n",
    "        cdef matrix_element_tree_s *flat_list_head, *current_element\n",
    "\n",
    "        # Flatten lower subtree\n",
    "        flat_list_head = self.subtree_to_list_flat(root.lower)\n",
    "\n",
    "        # If no lower elements exist, the head is the current element\n",
    "        if flat_list_head == NULL:\n",
    "            flat_list_head = root\n",
    "            root.lower = NULL\n",
    "\n",
    "        # Else move to the tail and add the subtree root\n",
    "        else:\n",
    "            current_element = flat_list_head\n",
    "            while current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            # Attach the element with the bidirectional pointers\n",
    "            current_element.higher = root\n",
    "            root.lower = current_element\n",
    "\n",
    "        # Flatten higher subtree and attach it to the tail of the flat list\n",
    "        root.higher = self.subtree_to_list_flat(root.higher)\n",
    "\n",
    "        # Attach the element with the bidirectional pointers\n",
    "        if root.higher != NULL:\n",
    "            root.higher.lower = root\n",
    "\n",
    "        return flat_list_head\n",
    "\n",
    "\n",
    "\n",
    "    cdef from_linked_list_to_python_list(self, matrix_element_tree_s * head):\n",
    "\n",
    "        data = []\n",
    "        column = []\n",
    "\n",
    "        while head != NULL:\n",
    "\n",
    "            if head.data != 0.0:\n",
    "                data.append(head.data)\n",
    "                column.append(head.column)\n",
    "\n",
    "            head = head.higher\n",
    "\n",
    "        return column, data\n",
    "\n",
    "\n",
    "\n",
    "    cdef subtree_free_memory(self, matrix_element_tree_s* root):\n",
    "        \"\"\"\n",
    "        The function frees all struct in the subtree whose root is passed as a parameter, root included \n",
    "        \n",
    "        :param root: tree root\n",
    "        \"\"\"\n",
    "\n",
    "        if root != NULL:\n",
    "            # If the root exists, open recursion\n",
    "            self.subtree_free_memory(root.higher)\n",
    "            self.subtree_free_memory(root.lower)\n",
    "\n",
    "            # Once the lower elements have been reached, start freeing from the bottom\n",
    "            PyMem_Free(root)\n",
    "\n",
    "\n",
    "\n",
    "    cdef list_free_memory(self, matrix_element_tree_s * head):\n",
    "        \"\"\"\n",
    "        The function frees all struct in the list whose head is passed as a parameter, head included \n",
    "        \n",
    "        :param head: list head\n",
    "        \"\"\"\n",
    "\n",
    "        if head != NULL:\n",
    "            # If the root exists, open recursion\n",
    "            self.subtree_free_memory(head.higher)\n",
    "\n",
    "            # Once the tail element have been reached, start freeing from them\n",
    "            PyMem_Free(head)\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s* build_tree_from_list_flat(self, matrix_element_tree_s* flat_list_head):\n",
    "        \"\"\"\n",
    "        The function builds a tree containing the passed data. This is the recursive function, the \n",
    "        data should be sorted by te caller\n",
    "        To ensure the tree is balanced, data is sorted according to the column   \n",
    "        \n",
    "        :param row: row in which to create new tree\n",
    "        :param column_vector: column coordinates \n",
    "        :param data_vector: cell data\n",
    "        \"\"\"\n",
    "\n",
    "        if flat_list_head == NULL:\n",
    "            return NULL\n",
    "\n",
    "\n",
    "        cdef long list_length = 0\n",
    "        cdef long middle_element_step = 0\n",
    "\n",
    "        cdef matrix_element_tree_s *current_element, *middleElement, *tree_root\n",
    "\n",
    "        current_element = flat_list_head\n",
    "        middleElement = flat_list_head\n",
    "\n",
    "        # Explore the flat list moving the middle elment every tho jumps\n",
    "        while current_element != NULL:\n",
    "            current_element = current_element.higher\n",
    "            list_length += 1\n",
    "            middle_element_step += 1\n",
    "\n",
    "            if middle_element_step == 2:\n",
    "                middleElement = middleElement.higher\n",
    "                middle_element_step = 0\n",
    "\n",
    "        tree_root = middleElement\n",
    "\n",
    "        # To execute the recursion it is necessary to cut the flat list\n",
    "        # The last of the lower elements will have to be a tail\n",
    "        if middleElement.lower != NULL:\n",
    "            middleElement.lower.higher = NULL\n",
    "\n",
    "            tree_root.lower = self.build_tree_from_list_flat(flat_list_head)\n",
    "\n",
    "\n",
    "        # The first of the higher elements will have to be a head\n",
    "        if middleElement.higher != NULL:\n",
    "            middleElement.higher.lower = NULL\n",
    "\n",
    "            tree_root.higher = self.build_tree_from_list_flat(middleElement.higher)\n",
    "\n",
    "\n",
    "        return tree_root\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s* topK_selection_from_list(self, matrix_element_tree_s* head, long TopK):\n",
    "        \"\"\"\n",
    "        The function selects the topK highest elements in the given list \n",
    "        \n",
    "        :param head: head of the list\n",
    "        :param TopK: number of highest elements to preserve\n",
    "        :return matrix_element_tree_s*: head of the new list\n",
    "        \"\"\"\n",
    "\n",
    "        cdef head_pointer_tree_s *vector_pointer_to_list_elements\n",
    "        cdef matrix_element_tree_s *current_element\n",
    "        cdef long list_length, index, selected_count\n",
    "\n",
    "        # Get list size\n",
    "        current_element = head\n",
    "        list_length = 0\n",
    "\n",
    "        while current_element != NULL:\n",
    "            list_length += 1\n",
    "            current_element = current_element.higher\n",
    "\n",
    "\n",
    "        # If list elements are not enough to perform a selection, return\n",
    "        if list_length < TopK:\n",
    "            return head\n",
    "\n",
    "        # Allocate vector that will be used for sorting\n",
    "        vector_pointer_to_list_elements = < head_pointer_tree_s *> PyMem_Malloc(list_length * sizeof(head_pointer_tree_s))\n",
    "\n",
    "        # Fill vector wit pointers to list elements\n",
    "        current_element = head\n",
    "        for index in range(list_length):\n",
    "            vector_pointer_to_list_elements[index].head = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "\n",
    "        # Sort array elements on their data field\n",
    "        qsort(vector_pointer_to_list_elements, list_length, sizeof(head_pointer_tree_s), compare_struct_on_data)\n",
    "\n",
    "        # Sort only the TopK according to their column field\n",
    "        # Sort is from lower to higher, therefore the elements to be considered are from len-topK to len\n",
    "        qsort(&vector_pointer_to_list_elements[list_length-TopK], TopK, sizeof(head_pointer_tree_s), compare_struct_on_column)\n",
    "\n",
    "\n",
    "        # Rebuild list attaching the consecutive elements\n",
    "        index = list_length-TopK\n",
    "\n",
    "        # Detach last TopK element from previous ones\n",
    "        vector_pointer_to_list_elements[index].head.lower = NULL\n",
    "\n",
    "        while index<list_length-1:\n",
    "            # Rearrange bidirectional pointers\n",
    "            vector_pointer_to_list_elements[index+1].head.lower = vector_pointer_to_list_elements[index].head\n",
    "            vector_pointer_to_list_elements[index].head.higher = vector_pointer_to_list_elements[index+1].head\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        # Last element in vector will be the hew head\n",
    "        vector_pointer_to_list_elements[list_length - 1].head.higher = NULL\n",
    "\n",
    "        # Get hew list head\n",
    "        current_element = vector_pointer_to_list_elements[list_length-TopK].head\n",
    "\n",
    "        # If there are exactly enough elements to reach TopK, index == 0 will be the tail\n",
    "        # Else, index will be the tail and the other elements will be removed\n",
    "        index = list_length - TopK - 1\n",
    "        if index > 0:\n",
    "\n",
    "            index -= 1\n",
    "            while index >= 0:\n",
    "                PyMem_Free(vector_pointer_to_list_elements[index].head)\n",
    "                index -= 1\n",
    "\n",
    "        # Free array\n",
    "        PyMem_Free(vector_pointer_to_list_elements)\n",
    "\n",
    "\n",
    "        return current_element\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            TEST FUNCTIONS\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "    cpdef test_list_tee_conversion(self, long row):\n",
    "        \"\"\"\n",
    "        The function tests the inner data structure conversion from tree to C linked list and back to tree\n",
    "        \n",
    "        :param row: row to use for testing\n",
    "        \"\"\"\n",
    "\n",
    "        cdef matrix_element_tree_s *head, *tree_root\n",
    "        cdef matrix_element_tree_s *current_element, *previous_element\n",
    "\n",
    "        head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "        current_element = head\n",
    "\n",
    "        cdef numElements_higher = 0\n",
    "        cdef numElements_lower = 0\n",
    "\n",
    "        while current_element != NULL:\n",
    "            numElements_higher += 1\n",
    "            previous_element = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "        current_element = previous_element\n",
    "        while current_element != NULL:\n",
    "            numElements_lower += 1\n",
    "            current_element = current_element.lower\n",
    "\n",
    "        assert numElements_higher == numElements_lower, 'Bidirectional linked list not consistent.' \\\n",
    "                                                        ' From head to tail element count is {}, from tail to head is {}'.format(\n",
    "                                                        numElements_higher, numElements_lower)\n",
    "\n",
    "        print(\"Bidirectional list link - Passed\")\n",
    "\n",
    "        column_original, data_original = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert numElements_higher == len(column_original), \\\n",
    "            'Data structure size inconsistent. LinkedList is {}, Python list is {}'.format(numElements_higher, len(column_original))\n",
    "\n",
    "        for index in range(len(column_original)-1):\n",
    "            assert column_original[index] < column_original[index+1],\\\n",
    "                'Columns not ordered correctly. Tree not flattened properly'\n",
    "\n",
    "        print(\"Bidirectional list ordering - Passed\")\n",
    "\n",
    "        # Transform list into tree and back into list, as it is easy to test\n",
    "        tree_root = self.build_tree_from_list_flat(head)\n",
    "        head = self.subtree_to_list_flat(tree_root)\n",
    "\n",
    "        cdef numElements_higher_after = 0\n",
    "        cdef numElements_lower_after = 0\n",
    "\n",
    "        current_element = head\n",
    "\n",
    "        while current_element != NULL:\n",
    "            numElements_higher_after += 1\n",
    "            previous_element = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "        current_element = previous_element\n",
    "        while current_element != NULL:\n",
    "            numElements_lower_after += 1\n",
    "            current_element = current_element.lower\n",
    "\n",
    "        print(\"Bidirectional list from tree link - Passed\")\n",
    "\n",
    "        assert numElements_higher_after == numElements_lower_after, \\\n",
    "            'Bidirectional linked list after tree construction not consistent. ' \\\n",
    "            'From head to tail element count is {}, from tail to head is {}'.format(\n",
    "            numElements_higher_after, numElements_lower_after)\n",
    "\n",
    "        assert numElements_higher == numElements_higher_after, \\\n",
    "            'Data structure size inconsistent. Original length is {}, after tree conversion is {}'.format(\n",
    "                numElements_higher, numElements_higher_after)\n",
    "\n",
    "        column_after_tree, data_after_tree = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert len(column_original) == len(column_after_tree), \\\n",
    "            'Data structure size inconsistent. Original length is {}, after tree conversion is {}'.format(\n",
    "                len(column_original), len(column_after_tree))\n",
    "\n",
    "        for index in range(len(column_original)):\n",
    "            assert column_original[index] == column_after_tree[index],\\\n",
    "                'After tree construction columns are not ordered properly'\n",
    "            assert data_original[index] == data_after_tree[index],\\\n",
    "                'After tree construction data content is changed'\n",
    "\n",
    "        print(\"Bidirectional list from tree ordering - Passed\")\n",
    "\n",
    "\n",
    "\n",
    "    cpdef test_topK_from_list_selection(self, long row, long topK):\n",
    "        \"\"\"\n",
    "        The function tests the topK selection from list\n",
    "        \n",
    "        :param row: row to use for testing\n",
    "        \"\"\"\n",
    "\n",
    "        cdef matrix_element_tree_s *head\n",
    "\n",
    "        head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "\n",
    "        column_original, data_original = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        head = self.topK_selection_from_list(head, topK)\n",
    "\n",
    "        column_topK, data_topK = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert len(column_topK) == len(data_topK),\\\n",
    "            \"TopK data and column lists have different length. Columns length is {}, data is {}\".format(len(column_topK), len(data_topK))\n",
    "        assert len(column_topK) <= topK,\\\n",
    "            \"TopK extracted list is longer than desired value. Desired is {}, while list is {}\".format(topK, len(column_topK))\n",
    "\n",
    "        print(\"TopK extracted length - Passed\")\n",
    "\n",
    "        # Sort with respect to the content to select topK\n",
    "        idx_sorted = np.argsort(data_original)\n",
    "        idx_sorted = np.flip(idx_sorted, axis=0)\n",
    "        top_k_idx = idx_sorted[0:topK]\n",
    "\n",
    "        column_topK_numpy = np.array(column_original)[top_k_idx]\n",
    "        data_topK_numpy = np.array(data_original)[top_k_idx]\n",
    "\n",
    "        # Sort with respect to the column to ensure it is ordered as the tree flattened list\n",
    "        idx_sorted = np.argsort(column_topK_numpy)\n",
    "        column_topK_numpy = column_topK_numpy[idx_sorted]\n",
    "        data_topK_numpy = data_topK_numpy[idx_sorted]\n",
    "\n",
    "\n",
    "        assert len(column_topK_numpy) <= len(column_topK),\\\n",
    "            \"TopK extracted list and numpy one have different length. Extracted list lenght is {}, while numpy is {}\".format(\n",
    "                len(column_topK_numpy), len(column_topK))\n",
    "\n",
    "\n",
    "        for index in range(len(column_topK)):\n",
    "\n",
    "            assert column_topK[index] == column_topK_numpy[index], \\\n",
    "                \"TopK extracted list and numpy one have different content at index {} as column value.\" \\\n",
    "                \" Extracted list lenght is {}, while numpy is {}\".format(index, column_topK[index], column_topK_numpy[index])\n",
    "\n",
    "            assert data_topK[index] == data_topK_numpy[index], \\\n",
    "                \"TopK extracted list and numpy one have different content at index {} as data value.\" \\\n",
    "                \" Extracted list lenght is {}, while numpy is {}\".format(index, data_topK[index], data_topK_numpy[index])\n",
    "\n",
    "        print(\"TopK extracted content - Passed\")\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            TRIANGULAR MATRIX\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "#from libc.stdlib cimport malloc\n",
    "# PyMem malloc and free are slightly faster than plain C equivalents as they optimize OS calls\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "\n",
    "#################################\n",
    "#################################       CLASS DECLARATION\n",
    "#################################\n",
    "\n",
    "cdef class Triangular_Matrix:\n",
    "\n",
    "    cdef long num_rows, num_cols\n",
    "    cdef int isSymmetric\n",
    "\n",
    "    cdef double** row_pointer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, long num_rows, int isSymmetric = False):\n",
    "\n",
    "        cdef int numRow, numCol\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_rows\n",
    "        self.isSymmetric = isSymmetric\n",
    "\n",
    "        self.row_pointer = <double **> PyMem_Malloc(self.num_rows * sizeof(double*))\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize all rows to empty\n",
    "        for numRow in range(self.num_rows):\n",
    "            self.row_pointer[numRow] = < double *> PyMem_Malloc((numRow+1) * sizeof(double))\n",
    "\n",
    "            for numCol in range(numRow+1):\n",
    "                self.row_pointer[numRow][numCol] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    def dealloc(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int numRow\n",
    "\n",
    "        # Free all rows memory\n",
    "        for numRow in range(self.num_rows):\n",
    "            PyMem_Free(self.row_pointer[numRow])\n",
    "\n",
    "        PyMem_Free(self.row_pointer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double add_value(self, long row, long col, double value):\n",
    "        \"\"\"\n",
    "        The function adds a value to the specified cell. A new cell is created if necessary.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :param value: value to add\n",
    "        :return double: resulting cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}),\"\n",
    "                             \" coordinates given are ({},{})\".format(\n",
    "                             self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        elif col > row:\n",
    "\n",
    "            if self.isSymmetric:\n",
    "                self.row_pointer[col][row] += value\n",
    "\n",
    "                return self.row_pointer[col][row]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Cell is in the upper triangular of the matrix,\"\n",
    "                                 \" current matrix is lower triangular.\"\n",
    "                                 \" Coordinates given are ({},{})\".format(row, col))\n",
    "        else:\n",
    "\n",
    "            self.row_pointer[row][col] += value\n",
    "\n",
    "            return self.row_pointer[row][col]\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double get_value(self, long row, long col):\n",
    "        \"\"\"\n",
    "        The function returns the value of the specified cell.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :return double: cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        elif col > row:\n",
    "\n",
    "            if self.isSymmetric:\n",
    "                return self.row_pointer[col][row]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Cell is in the upper triangular of the matrix,\"\n",
    "                                 \" current matrix is lower triangular.\"\n",
    "                                 \" Coordinates given are ({},{})\".format(row, col))\n",
    "        else:\n",
    "\n",
    "            return self.row_pointer[row][col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef get_scipy_csr(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function returns the current sparse matrix as a scipy_csr object         \n",
    "   \n",
    "        :return double: scipy_csr object\n",
    "        \"\"\"\n",
    "        cdef int terminate\n",
    "        cdef long row, col, index\n",
    "\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] currentRowArray = clone(template_zero, self.num_cols, zero=True)\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] currentRowArray_np\n",
    "\n",
    "\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        # Loop the rows\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            #Always set indptr\n",
    "            indptr.append(len(data))\n",
    "\n",
    "            # Fill RowArray\n",
    "            for col in range(self.num_cols):\n",
    "\n",
    "                if col <= row:\n",
    "                    currentRowArray[col] = self.row_pointer[row][col]\n",
    "                else:\n",
    "                    if self.isSymmetric:\n",
    "                        currentRowArray[col] = self.row_pointer[col][row]\n",
    "                    else:\n",
    "                        currentRowArray[col] = 0.0\n",
    "\n",
    "\n",
    "            if TopK:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                currentRowArray_np = - np.array(currentRowArray)\n",
    "                #\n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(currentRowArray_np, TopK-1)[0:TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(currentRowArray_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "                for index in range(len(top_k_idx)):\n",
    "\n",
    "                    col = top_k_idx[index]\n",
    "\n",
    "                    if currentRowArray[col] != 0.0:\n",
    "                        indices.append(col)\n",
    "                        data.append(currentRowArray[col])\n",
    "\n",
    "            else:\n",
    "\n",
    "                for index in range(self.num_cols):\n",
    "\n",
    "                    if currentRowArray[index] != 0.0:\n",
    "                        indices.append(index)\n",
    "                        data.append(currentRowArray[index])\n",
    "\n",
    "\n",
    "        #Set terminal indptr\n",
    "        indptr.append(len(data))\n",
    "\n",
    "        return sps.csr_matrix((data, indices, indptr), shape=(self.num_rows, self.num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68e4efaf73926fcde7d0ac2ea52479b2bbdeb39a"
   },
   "source": [
    "# Load Interaction Data - Load Content Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARK: - Load interaction data\n",
      "The number of interactions is 1211792\n",
      "[(0, 14301, 1), (0, 8360, 1), (0, 12844, 1), (0, 18397, 1), (0, 1220, 1), (1, 18466, 1), (1, 16782, 1), (1, 7545, 1), (1, 8248, 1)]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[14301, 8360, 12844, 18397, 1220, 18466, 16782, 7545, 8248, 16866]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "URM_all shape\n",
      "(50446, 20635)\n",
      "start_split\n",
      "URM_train len\n",
      "806421\n",
      "URM_test len\n",
      "1211791\n",
      "MARK: - Load Content Data\n",
      "[(0, 6306, 449, 167), (1, 12085, 4903, 185), (2, 1885, 6358, 201), (3, 3989, 1150, 263), (4, 11633, 4447, 96), (5, 9666, 6096, 221), (6, 4426, 2029, 151), (7, 8960, 5396, 192), (8, 11700, 1884, 206)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[6306, 12085, 1885, 3989, 11633, 9666, 4426, 8960, 11700, 11667]\n",
      "[449, 4903, 6358, 1150, 4447, 6096, 2029, 5396, 1884, 2926]\n",
      "[167, 185, 201, 263, 96, 221, 151, 192, 206, 206]\n"
     ]
    }
   ],
   "source": [
    "print(\"MARK: - Load interaction data\")\n",
    "\n",
    "URM_path = \"../input/train.csv\"\n",
    "URM_file = open(URM_path, 'r')\n",
    "URM_file.seek(0)\n",
    "numberInteractions = 0\n",
    "\n",
    "for _ in URM_file:\n",
    "    numberInteractions += 1\n",
    "\n",
    "print(\"The number of interactions is {}\".format(numberInteractions))\n",
    "\n",
    "def rowSplitTrain(rowString):\n",
    "    split = rowString.split(\",\")\n",
    "    split[1] = split[1].replace(\"\\n\", \"\")\n",
    "    split.append(1)\n",
    "    try:\n",
    "        split[0] = int(split[0])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        split[1] = int(split[1])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return tuple(split)\n",
    "\n",
    "\n",
    "URM_file.seek(0)\n",
    "URM_tuples = []\n",
    "\n",
    "for line in URM_file:\n",
    "    URM_tuples.append(rowSplitTrain(line))\n",
    "\n",
    "print(URM_tuples[1:10])\n",
    "\n",
    "playlist_list, track_list, rating_list = zip(*URM_tuples)\n",
    "\n",
    "playlist_list = list(playlist_list)[1:]\n",
    "track_list = list(track_list)[1:]\n",
    "rating_list = list(rating_list)[1:]\n",
    "\n",
    "print(playlist_list[0:10])\n",
    "print(track_list[0:10])\n",
    "print(rating_list[0:10])\n",
    "\n",
    "playlist_list_unique = list(set(playlist_list))\n",
    "track_list_unique = list(set(track_list))\n",
    "\n",
    "print(playlist_list_unique[0:10])\n",
    "print(track_list_unique[0:10])\n",
    "\n",
    "URM_all = sps.csr_matrix((rating_list, (playlist_list, track_list)))\n",
    "print('URM_all shape')\n",
    "print(URM_all.shape)\n",
    "URM_train, URM_test = train_test_holdout_adjusted(URM_all)\n",
    "\n",
    "print(\"URM_train len\")\n",
    "print(len(URM_train.indices))\n",
    "\n",
    "print(\"URM_test len\")\n",
    "print(len(URM_test.indices))\n",
    "\n",
    "# MARK: - Load content data\n",
    "\n",
    "print(\"MARK: - Load Content Data\")\n",
    "\n",
    "ICM_path = \"../input/tracks.csv\"\n",
    "ICM_file = open(ICM_path, 'r')\n",
    "ICM_file.seek(0)\n",
    "\n",
    "def rowSplitTracks(rowString):\n",
    "    split = rowString.split(\",\")\n",
    "    try:\n",
    "        split[0] = int(split[0])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        split[1] = int(split[1])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        split[2] = int(split[2])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        split[3] = int(split[3])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return tuple(split)\n",
    "\n",
    "ICM_tuples = []\n",
    "\n",
    "for line in ICM_file:\n",
    "    ICM_tuples.append(rowSplitTracks(line))\n",
    "\n",
    "print(ICM_tuples[1:10])\n",
    "\n",
    "track_list_icm, album_list_icm, artist_list_icm, duration_list_icm = zip(*ICM_tuples)\n",
    "\n",
    "track_list_icm = list(track_list_icm)[1:]\n",
    "album_list_icm = list(album_list_icm)[1:]\n",
    "artist_list_icm = list(artist_list_icm)[1:]\n",
    "duration_list_icm = list(duration_list_icm)[1:]\n",
    "\n",
    "print(track_list_icm[0:10])\n",
    "print(album_list_icm[0:10])\n",
    "print(artist_list_icm[0:10])\n",
    "print(duration_list_icm[0:10])\n",
    "\n",
    "ones = np.ones(len(track_list_icm))\n",
    "\n",
    "ICM_album = sps.coo_matrix((ones, (track_list_icm, album_list_icm)))\n",
    "ICM_album = ICM_album.tocsr()\n",
    "\n",
    "ICM_artist = sps.coo_matrix((ones, (track_list_icm, artist_list_icm)))\n",
    "ICM_artist = ICM_artist.tocsr()\n",
    "\n",
    "ICM_duration = sps.coo_matrix((ones, (track_list_icm, duration_list_icm)))\n",
    "ICM_duration = ICM_duration.tocsr()\n",
    "\n",
    "ICM = sps.hstack([ICM_album, ICM_artist, ICM_duration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f91a81ed7c8d6326f7b26f93b3fd07766303836"
   },
   "source": [
    "# DEF - Recommender - Hybrid ICF + ICBF + UCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "d934d0394fe631d73bd73e0a3ba163357925a973"
   },
   "outputs": [],
   "source": [
    "class hybridRecommender(object):\n",
    "\n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "\n",
    "    def fit(self, topK=50, shrink=100, normalize=True, similarity=\"cosine\"):\n",
    "        similarity_object_track_ucf = Compute_Similarity_Cython(self.URM.T, shrink=shrink, topK=topK, normalize=normalize, similarity=similarity)\n",
    "        similarity_object_track_icf = Compute_Similarity_Cython(self.URM, shrink=shrink, topK=topK, normalize=normalize, similarity=similarity)\n",
    "        similarity_object_track_icbf = Compute_Similarity_Cython(self.ICM.T, shrink=shrink, topK=topK, normalize=normalize, similarity=similarity)\n",
    "\n",
    "        self.W_sparse_ucf = similarity_object_track_ucf.compute_similarity()\n",
    "        self.W_sparse_icf = similarity_object_track_icf.compute_similarity()\n",
    "        self.W_sparse_icbf = similarity_object_track_icbf.compute_similarity()\n",
    "\n",
    "    def recommend(self, playlist_id, at=10, exclude_duplicates=True):\n",
    "        # compute the scores using the dot product\n",
    "\n",
    "        playlist_profile_u = self.W_sparse_ucf[playlist_id]\n",
    "        playlist_profile_i = self.URM[playlist_id]\n",
    "        scores_ucf = playlist_profile_u.dot(self.URM).toarray().ravel()\n",
    "        scores_icf = playlist_profile_i.dot(self.W_sparse_icf).toarray().ravel()\n",
    "        scores_icbf = playlist_profile_i.dot(self.W_sparse_icbf).toarray().ravel()\n",
    "\n",
    "        scores_norm_ucf = (scores_ucf-scores_ucf.min() + 1e-6)/(scores_ucf.max()-scores_ucf.min() + 1e-6)\n",
    "        scores_norm_icf = (scores_icf-scores_icf.min() + 1e-6)/(scores_icf.max()-scores_icf.min() + 1e-6)\n",
    "        scores_norm_icbf = (scores_icbf-scores_icbf.min() + 1e-6)/(scores_icbf.max()-scores_icbf.min() + 1e-6)\n",
    "\n",
    "        scores =  scores_norm_ucf * 0.4 + scores_norm_icf * 0.45 + scores_norm_icbf * 0.15\n",
    "        #scores = scores_norm_icf\n",
    "        \n",
    "        if exclude_duplicates:\n",
    "            scores = self.filter_seen(playlist_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "\n",
    "        return ranking[:at]\n",
    "\n",
    "\n",
    "    def filter_seen(self, playlist_id, scores):\n",
    "        start_pos = self.URM.indptr[playlist_id]\n",
    "        end_pos = self.URM.indptr[playlist_id+1]\n",
    "\n",
    "        playlist_profile = self.URM.indices[start_pos:end_pos]\n",
    "\n",
    "        scores[playlist_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46dd70db264597ad13391b03d0cb3390d7dca739"
   },
   "source": [
    "# DEF - Recommender - BPR Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "7f9eae5d9e062fccbe8a8bb98301a55f3c09dcb2"
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR(Recommender):\n",
    "    \"\"\"\n",
    "    This class is a python porting of the BPRSLIM algorithm in MyMediaLite written in C#\n",
    "    The code is identical with no optimizations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URM_train, ICM, lambda_i = 0.0025, lambda_j = 0.00025, learning_rate = 0.05):\n",
    "        super(SLIM_BPR, self).__init__()\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "        self.ICM = ICM\n",
    "        self.n_users = URM_train.shape[0]\n",
    "        self.n_items = URM_train.shape[1]\n",
    "        self.lambda_i = lambda_i\n",
    "        self.lambda_j = lambda_j\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.normalize = False\n",
    "        self.sparse_weights = False\n",
    "\n",
    "\n",
    "    def updateFactors(self, user_id, pos_item_id, neg_item_id):\n",
    "\n",
    "        # Calculate current predicted score\n",
    "        userSeenItems = self.URM_train[user_id].indices\n",
    "        prediction = 0\n",
    "\n",
    "        for userSeenItem in userSeenItems:\n",
    "            prediction += self.S[pos_item_id, userSeenItem] - self.S[neg_item_id, userSeenItem]\n",
    "\n",
    "\n",
    "        x_uij = prediction\n",
    "        logisticFunction = expit(-x_uij)\n",
    "\n",
    "        # Update similarities for all items except those sampled\n",
    "        for userSeenItem in userSeenItems:\n",
    "\n",
    "            # For positive item is PLUS logistic minus lambda*S\n",
    "            if(pos_item_id != userSeenItem):\n",
    "                update = logisticFunction - self.lambda_i*self.S[pos_item_id, userSeenItem]\n",
    "                self.S[pos_item_id, userSeenItem] += self.learning_rate*update\n",
    "\n",
    "            # For positive item is MINUS logistic minus lambda*S\n",
    "            if (neg_item_id != userSeenItem):\n",
    "                update = - logisticFunction - self.lambda_j*self.S[neg_item_id, userSeenItem]\n",
    "                self.S[neg_item_id, userSeenItem] += self.learning_rate*update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs=15):\n",
    "        \"\"\"\n",
    "        Train SLIM wit BPR. If the model was already trained, overwrites matrix S\n",
    "        :param epochs:\n",
    "        :return: -\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize similarity with random values and zero-out diagonal\n",
    "        self.S = np.random.random((self.n_items, self.n_items)).astype('float32')\n",
    "        self.S[np.arange(self.n_items),np.arange(self.n_items)] = 0\n",
    "\n",
    "        start_time_train = time.time()\n",
    "\n",
    "        for currentEpoch in range(epochs):\n",
    "\n",
    "            start_time_epoch = time.time()\n",
    "\n",
    "            self.epochIteration()\n",
    "            print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch+1, epochs, float(time.time()-start_time_epoch)/60))\n",
    "\n",
    "        print(\"Train completed in {:.2f} minutes\".format(float(time.time()-start_time_train)/60))\n",
    "\n",
    "        # The similarity matrix is learnt row-wise\n",
    "        # To be used in the product URM*S must be transposed to be column-wise\n",
    "        self.W = self.S.T\n",
    "\n",
    "        del self.S\n",
    "\n",
    "\n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = self.URM_train.nnz\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for numSample in range(numPositiveIteractions):\n",
    "\n",
    "            user_id, pos_item_id, neg_item_id = self.sampleTriple()\n",
    "            self.updateFactors(user_id, pos_item_id, neg_item_id)\n",
    "\n",
    "            if(numSample % 5000 == 0):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.4f} seconds\".format(numSample,\n",
    "                                  100.0* float(numSample)/numPositiveIteractions,\n",
    "                                  time.time()-start_time))\n",
    "\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sampleUser(self):\n",
    "        \"\"\"\n",
    "        Sample a user that has viewed at least one and not all items\n",
    "        :return: user_id\n",
    "        \"\"\"\n",
    "        while(True):\n",
    "\n",
    "            user_id = np.random.randint(0, self.n_users)\n",
    "            numSeenItems = self.URM_train[user_id].nnz\n",
    "\n",
    "            if(numSeenItems >0 and numSeenItems<self.n_items):\n",
    "                return user_id\n",
    "\n",
    "\n",
    "\n",
    "    def sampleItemPair(self, user_id):\n",
    "        \"\"\"\n",
    "        Returns for the given user a random seen item and a random not seen item\n",
    "        :param user_id:\n",
    "        :return: pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        userSeenItems = self.URM_train[user_id].indices\n",
    "\n",
    "        pos_item_id = userSeenItems[np.random.randint(0,len(userSeenItems))]\n",
    "\n",
    "        while(True):\n",
    "\n",
    "            neg_item_id = np.random.randint(0, self.n_items)\n",
    "\n",
    "            if(neg_item_id not in userSeenItems):\n",
    "                return pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "    def sampleTriple(self):\n",
    "        \"\"\"\n",
    "        Randomly samples a user and then samples randomly a seen and not seen item\n",
    "        :return: user_id, pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        user_id = self.sampleUser()\n",
    "        pos_item_id, neg_item_id = self.sampleItemPair(user_id)\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2d11a368952839bf42c22fff67b052695cb95dc"
   },
   "source": [
    "# DEF - Recommender - BPR Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "c34f04cd022cf34ff5c0c77b7adb4a15bf9e2d64"
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR_Cython(SimilarityMatrixRecommender, Recommender, Incremental_Training_Early_Stopping):\n",
    "\n",
    "    RECOMMENDER_NAME = \"SLIM_BPR_Recommender + \"\n",
    "\n",
    "\n",
    "    def __init__(self, URM_train, ICM, positive_threshold=4, URM_validation = None,\n",
    "                 recompile_cython = False, final_model_sparse_weights = True, train_with_sparse_weights = False,\n",
    "                 symmetric = True):\n",
    "\n",
    "\n",
    "        super(SLIM_BPR_Cython, self).__init__()\n",
    "\n",
    "\n",
    "        self.URM_train = URM_train.copy()\n",
    "        self.ICM = ICM\n",
    "        self.n_users = URM_train.shape[0]\n",
    "        self.n_items = URM_train.shape[1]\n",
    "        self.normalize = False\n",
    "        self.positive_threshold = positive_threshold\n",
    "\n",
    "        self.train_with_sparse_weights = train_with_sparse_weights\n",
    "        self.sparse_weights = final_model_sparse_weights\n",
    "\n",
    "        if URM_validation is not None:\n",
    "            self.URM_validation = URM_validation.copy()\n",
    "        else:\n",
    "            self.URM_validation = None\n",
    "\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.sparse_weights = True\n",
    "\n",
    "\n",
    "        self.URM_mask = self.URM_train.copy()\n",
    "\n",
    "        self.URM_mask.data = self.URM_mask.data >= self.positive_threshold\n",
    "        self.URM_mask.eliminate_zeros()\n",
    "\n",
    "        assert self.URM_mask.nnz > 0, \"MatrixFactorization_Cython: URM_train_positive is empty, positive threshold is too high\"\n",
    "\n",
    "\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        if not self.train_with_sparse_weights:\n",
    "\n",
    "            n_items = URM_train.shape[1]\n",
    "            requiredGB = 8 * n_items**2 / 1e+06\n",
    "\n",
    "            if symmetric:\n",
    "                requiredGB /=2\n",
    "\n",
    "            print(\"SLIM_BPR_Cython: Estimated memory required for similarity matrix of {} items is {:.2f} MB\".format(n_items, requiredGB))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if recompile_cython:\n",
    "            print(\"Compiling in Cython\")\n",
    "            self.runCompilationScript()\n",
    "            print(\"Compilation Complete\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs=300, logFile=None,\n",
    "            batch_size = 1000, lambda_i = 0.0, lambda_j = 0.0, learning_rate = 1e-4, topK = 200,\n",
    "            sgd_mode='adagrad', gamma=0.995, beta_1=0.9, beta_2=0.999,\n",
    "            stop_on_validation = False, lower_validatons_allowed = 5, validation_metric = \"MAP\",\n",
    "            evaluator_object = None, validation_every_n = 1):\n",
    "        \n",
    "        # UCF + ICF + ICBF\n",
    "        \n",
    "        similarity_object_track_ucf = Compute_Similarity_Cython(self.URM_train.T, shrink=8, topK=topK, normalize=True, similarity=\"cosine\")\n",
    "        similarity_object_track_icf = Compute_Similarity_Cython(self.URM_train, shrink=8, topK=topK, normalize=True, similarity=\"cosine\")\n",
    "        similarity_object_track_icbf = Compute_Similarity_Cython(self.ICM.T, shrink=8, topK=topK, normalize=True, similarity=\"cosine\")\n",
    "\n",
    "        self.W_sparse_ucf = similarity_object_track_ucf.compute_similarity()\n",
    "        self.W_sparse_icf = similarity_object_track_icf.compute_similarity()\n",
    "        self.W_sparse_icbf = similarity_object_track_icbf.compute_similarity()\n",
    "        \n",
    "        self.W_sparse_item = np.add(self.W_sparse_icf, self.W_sparse_icbf)\n",
    "\n",
    "        # Select only positive interactions\n",
    "        URM_train_positive = self.URM_train.copy()\n",
    "\n",
    "        URM_train_positive.data = URM_train_positive.data >= self.positive_threshold\n",
    "        URM_train_positive.eliminate_zeros()\n",
    "\n",
    "        self.sgd_mode = sgd_mode\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "        self.cythonEpoch = SLIM_BPR_Cython_Epoch(self.URM_mask,\n",
    "                                                 train_with_sparse_weights = self.train_with_sparse_weights,\n",
    "                                                 final_model_sparse_weights = self.sparse_weights,\n",
    "                                                 topK=topK,\n",
    "                                                 learning_rate=learning_rate,\n",
    "                                                 li_reg = lambda_i,\n",
    "                                                 lj_reg = lambda_j,\n",
    "                                                 batch_size=1,\n",
    "                                                 symmetric = self.symmetric,\n",
    "                                                 sgd_mode = sgd_mode,\n",
    "                                                 gamma=gamma,\n",
    "                                                 beta_1=beta_1,\n",
    "                                                 beta_2=beta_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if(topK != False and topK<1):\n",
    "            raise ValueError(\"TopK not valid. Acceptable values are either False or a positive integer value. Provided value was '{}'\".format(topK))\n",
    "        self.topK = topK\n",
    "\n",
    "        if validation_every_n is not None:\n",
    "            self.validation_every_n = validation_every_n\n",
    "        else:\n",
    "            self.validation_every_n = np.inf\n",
    "\n",
    "        if evaluator_object is None and stop_on_validation:\n",
    "            evaluator_object = SequentialEvaluator(self.URM_validation, [5])\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_i = lambda_i\n",
    "        self.lambda_j = lambda_j\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        self._train_with_early_stopping(epochs, validation_every_n, stop_on_validation,\n",
    "                                    validation_metric, lower_validatons_allowed, evaluator_object,\n",
    "                                    algorithm_name = self.RECOMMENDER_NAME)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.get_S_incremental_and_set_W()\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "    def _initialize_incremental_model(self):\n",
    "        self.S_incremental = self.cythonEpoch.get_S()\n",
    "        self.S_best = self.S_incremental.copy()\n",
    "\n",
    "\n",
    "    def _update_incremental_model(self):\n",
    "        self.get_S_incremental_and_set_W()\n",
    "\n",
    "\n",
    "    def _update_best_model(self):\n",
    "        self.S_best = self.S_incremental.copy()\n",
    "\n",
    "    def _run_epoch(self, num_epoch):\n",
    "       self.cythonEpoch.epochIteration_Cython()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_S_incremental_and_set_W(self):\n",
    "\n",
    "        self.S_incremental = self.cythonEpoch.get_S()\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.W_sparse = self.S_incremental\n",
    "        else:\n",
    "            if self.sparse_weights:\n",
    "                self.W_sparse = similarityMatrixTopK(self.S_incremental, k = self.topK)\n",
    "            else:\n",
    "                self.W = self.S_incremental\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def writeCurrentConfig(self, currentEpoch, results_run, logFile):\n",
    "\n",
    "        current_config = {'lambda_i': self.lambda_i,\n",
    "                          'lambda_j': self.lambda_j,\n",
    "                          'batch_size': self.batch_size,\n",
    "                          'learn_rate': self.learning_rate,\n",
    "                          'topK_similarity': self.topK,\n",
    "                          'epoch': currentEpoch}\n",
    "\n",
    "        print(\"Test case: {}\\nResults {}\\n\".format(current_config, results_run))\n",
    "        # print(\"Weights: {}\\n\".format(str(list(self.weights))))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (logFile != None):\n",
    "            logFile.write(\"Test case: {}, Results {}\\n\".format(current_config, results_run))\n",
    "            # logFile.write(\"Weights: {}\\n\".format(str(list(self.weights))))\n",
    "            logFile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cfd1d0e72c9b0e49170be17a178bcc273c929bf"
   },
   "source": [
    "# DEF - Utils - Cython Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "fb3bf46b764c9c2e458830394876208573297c77"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import time, sys\n",
    "import subprocess\n",
    "\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "from libc.math cimport exp, sqrt\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            idx_sorted = np.argsort(column_data)  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[top_k_idx])\n",
    "            rows_indices.extend(column_row_index[top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse\n",
    "\n",
    "cdef struct BPR_sample:\n",
    "    long user\n",
    "    long pos_item\n",
    "    long neg_item\n",
    "    long seen_items_start_pos\n",
    "    long seen_items_end_pos\n",
    "\n",
    "\n",
    "\n",
    "cdef class SLIM_BPR_Cython_Epoch:\n",
    "\n",
    "    cdef int n_users\n",
    "    cdef int n_items\n",
    "    cdef int numPositiveIteractions\n",
    "    cdef int topK\n",
    "    cdef int symmetric, train_with_sparse_weights, final_model_sparse_weights\n",
    "\n",
    "    cdef double learning_rate, li_reg, lj_reg\n",
    "\n",
    "    cdef int batch_size\n",
    "\n",
    "\n",
    "    cdef int[:] URM_mask_indices, URM_mask_indptr\n",
    "\n",
    "    cdef Sparse_Matrix_Tree_CSR S_sparse\n",
    "    cdef Triangular_Matrix S_symmetric\n",
    "    cdef double[:,:] S_dense\n",
    "\n",
    "\n",
    "    # Adaptive gradient\n",
    "\n",
    "    cdef int useAdaGrad, useRmsprop, useAdam\n",
    "\n",
    "    cdef double [:] sgd_cache_I\n",
    "    cdef double gamma\n",
    "\n",
    "    cdef double [:] sgd_cache_I_momentum_1, sgd_cache_I_momentum_2\n",
    "    cdef double beta_1, beta_2, beta_1_power_t, beta_2_power_t\n",
    "    cdef double momentum_1, momentum_2\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, URM_mask,\n",
    "                 train_with_sparse_weights = False,\n",
    "                 final_model_sparse_weights = True,\n",
    "                 learning_rate = 0.01, li_reg = 0.0, lj_reg = 0.0,\n",
    "                 batch_size = 1, topK = 150, symmetric = True,\n",
    "                 sgd_mode='adam', gamma=0.995, beta_1=0.9, beta_2=0.999):\n",
    "\n",
    "        super(SLIM_BPR_Cython_Epoch, self).__init__()\n",
    "\n",
    "        URM_mask = check_matrix(URM_mask, 'csr')\n",
    "\n",
    "        self.numPositiveIteractions = int(URM_mask.nnz * 1)\n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "        self.topK = topK\n",
    "        self.learning_rate = learning_rate\n",
    "        self.li_reg = li_reg\n",
    "        self.lj_reg = lj_reg\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "        if train_with_sparse_weights:\n",
    "            symmetric = False\n",
    "\n",
    "        self.train_with_sparse_weights = train_with_sparse_weights\n",
    "        self.final_model_sparse_weights = final_model_sparse_weights\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        self.URM_mask_indices = np.array(URM_mask.indices, dtype=np.int32)\n",
    "        self.URM_mask_indptr = np.array(URM_mask.indptr, dtype=np.int32)\n",
    "\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.S_sparse = Sparse_Matrix_Tree_CSR(self.n_items, self.n_items)\n",
    "\n",
    "        elif self.symmetric:\n",
    "            self.S_symmetric = Triangular_Matrix(self.n_items, isSymmetric = True)\n",
    "        else:\n",
    "            self.S_dense = np.zeros((self.n_items, self.n_items), dtype=np.float64)\n",
    "\n",
    "\n",
    "        self.useAdaGrad = False\n",
    "        self.useRmsprop = False\n",
    "        self.useAdam = False\n",
    "\n",
    "\n",
    "        if sgd_mode=='adagrad':\n",
    "            self.useAdaGrad = True\n",
    "            self.sgd_cache_I = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "        elif sgd_mode=='rmsprop':\n",
    "            self.useRmsprop = True\n",
    "            self.sgd_cache_I = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "            # Gamma default value suggested by Hinton\n",
    "            # self.gamma = 0.9\n",
    "            self.gamma = gamma\n",
    "\n",
    "        elif sgd_mode=='adam':\n",
    "            self.useAdam = True\n",
    "            self.sgd_cache_I_momentum_1 = np.zeros((self.n_items), dtype=np.float64)\n",
    "            self.sgd_cache_I_momentum_2 = np.zeros((self.n_items), dtype=np.float64)\n",
    "\n",
    "            # Default value suggested by the original paper\n",
    "            # beta_1=0.9, beta_2=0.999\n",
    "            self.beta_1 = beta_1\n",
    "            self.beta_2 = beta_2\n",
    "            self.beta_1_power_t = beta_1\n",
    "            self.beta_2_power_t = beta_2\n",
    "\n",
    "        elif sgd_mode=='sgd':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"SGD_mode not valid. Acceptable values are: 'sgd', 'adagrad', 'rmsprop', 'adam'. Provided value was '{}'\".format(\n",
    "                    sgd_mode))\n",
    "\n",
    "\n",
    "    def __dealloc__(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            self.S_sparse.dealloc()\n",
    "\n",
    "        elif self.symmetric:\n",
    "            self.S_symmetric.dealloc()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def epochIteration_Cython(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long totalNumberOfBatch = int(self.numPositiveIteractions / self.batch_size) + 1\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long start_time_batch = time.time()\n",
    "\n",
    "        cdef BPR_sample sample\n",
    "        cdef long i, j\n",
    "        cdef long index, seenItem, numCurrentBatch, itemId\n",
    "        cdef double x_uij, gradient, loss = 0.0\n",
    "        cdef double gradient_update\n",
    "\n",
    "        cdef int numSeenItems\n",
    "        cdef int printStep\n",
    "\n",
    "        if self.train_with_sparse_weights:\n",
    "            printStep = 500000\n",
    "        else:\n",
    "            printStep = 5000000\n",
    "\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for numCurrentBatch in range(totalNumberOfBatch):\n",
    "\n",
    "            sample = self.sampleBPR_Cython()\n",
    "\n",
    "            i = sample.pos_item\n",
    "            j = sample.neg_item\n",
    "\n",
    "            x_uij = 0.0\n",
    "\n",
    "            # The difference is computed on the user_seen items\n",
    "\n",
    "            index = 0\n",
    "            while index <  sample.seen_items_end_pos - sample.seen_items_start_pos:\n",
    "\n",
    "                seenItem = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "                index +=1\n",
    "\n",
    "                if self.train_with_sparse_weights:\n",
    "                   x_uij += self.S_sparse.get_value(i, seenItem) - self.S_sparse.get_value(j, seenItem)\n",
    "\n",
    "                elif self.symmetric:\n",
    "                    x_uij += self.S_symmetric.get_value(i, seenItem) - self.S_symmetric.get_value(j, seenItem)\n",
    "\n",
    "                else:\n",
    "                    x_uij += self.S_dense[i, seenItem] - self.S_dense[j, seenItem]\n",
    "\n",
    "\n",
    "            gradient = 1 / (1 + exp(x_uij))\n",
    "            loss += x_uij**2\n",
    "\n",
    "\n",
    "            # if self.useAdaGrad:\n",
    "            #     cacheUpdate = gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            # elif self.useRmsprop:\n",
    "            #     cacheUpdate = sgd_cache[i] * gamma + (1 - gamma) * gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] = cacheUpdate\n",
    "            #     sgd_cache[j] = cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            #\n",
    "            # #######################################\n",
    "\n",
    "\n",
    "            if self.useAdaGrad:\n",
    "                self.sgd_cache_I[i] += gradient ** 2\n",
    "                self.sgd_cache_I[j] += gradient ** 2\n",
    "\n",
    "                gradient_update = gradient / (sqrt(self.sgd_cache_I[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            elif self.useRmsprop:\n",
    "                self.sgd_cache_I[i] = self.sgd_cache_I[i] * self.gamma + (1 - self.gamma) * gradient ** 2\n",
    "                self.sgd_cache_I[j] = self.sgd_cache_I[j] * self.gamma + (1 - self.gamma) * gradient ** 2\n",
    "\n",
    "                gradient_update = gradient / (sqrt(self.sgd_cache_I[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            elif self.useAdam:\n",
    "\n",
    "                self.sgd_cache_I_momentum_1[i] = \\\n",
    "                    self.sgd_cache_I_momentum_1[i] * self.beta_1 + (1 - self.beta_1) * gradient\n",
    "\n",
    "                self.sgd_cache_I_momentum_2[i] = \\\n",
    "                    self.sgd_cache_I_momentum_2[i] * self.beta_2 + (1 - self.beta_2) * gradient**2\n",
    "\n",
    "\n",
    "                self.momentum_1 = self.sgd_cache_I_momentum_1[i]/ (1 - self.beta_1_power_t)\n",
    "                self.momentum_2 = self.sgd_cache_I_momentum_2[i]/ (1 - self.beta_2_power_t)\n",
    "\n",
    "                gradient_update = self.momentum_1/ (sqrt(self.momentum_2) + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                self.sgd_cache_I_momentum_1[j] = \\\n",
    "                    self.sgd_cache_I_momentum_1[j] * self.beta_1 + (1 - self.beta_1) * gradient\n",
    "\n",
    "                self.sgd_cache_I_momentum_2[j] = \\\n",
    "                    self.sgd_cache_I_momentum_2[j] * self.beta_2 + (1 - self.beta_2) * gradient**2\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                gradient_update = gradient\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            index = 0\n",
    "            while index < sample.seen_items_end_pos - sample.seen_items_start_pos:\n",
    "\n",
    "                seenItem = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "                index +=1\n",
    "\n",
    "                if self.train_with_sparse_weights:\n",
    "                    # Since the sparse matrix is slower compared to the others\n",
    "                    # If no reg is required, avoid accessing it\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        if self.li_reg!= 0.0:\n",
    "                            self.S_sparse.add_value(i, seenItem, self.learning_rate * (gradient_update - self.li_reg * self.S_sparse.get_value(i, seenItem)))\n",
    "                        else:\n",
    "                            self.S_sparse.add_value(i, seenItem, self.learning_rate * gradient_update)\n",
    "\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        if self.lj_reg!= 0.0:\n",
    "                            self.S_sparse.add_value(j, seenItem, -self.learning_rate * (gradient_update - self.lj_reg * self.S_sparse.get_value(j, seenItem)))\n",
    "                        else:\n",
    "                            self.S_sparse.add_value(j, seenItem, -self.learning_rate * gradient_update)\n",
    "\n",
    "\n",
    "                elif self.symmetric:\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        self.S_symmetric.add_value(i, seenItem, self.learning_rate * (gradient_update - self.li_reg * self.S_symmetric.get_value(i, seenItem)))\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        self.S_symmetric.add_value(j, seenItem, -self.learning_rate * (gradient_update - self.lj_reg * self.S_symmetric.get_value(j, seenItem)))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if seenItem != i:\n",
    "                        self.S_dense[i, seenItem] += self.learning_rate * (gradient_update - self.li_reg * self.S_dense[i, seenItem])\n",
    "\n",
    "                    if seenItem != j:\n",
    "                        self.S_dense[j, seenItem] -= self.learning_rate * (gradient_update - self.lj_reg * self.S_dense[j, seenItem])\n",
    "\n",
    "\n",
    "\n",
    "            # Exponentiation of beta at the end of each sample\n",
    "            if self.useAdam:\n",
    "\n",
    "                self.beta_1_power_t *= self.beta_1\n",
    "                self.beta_2_power_t *= self.beta_2\n",
    "\n",
    "\n",
    "\n",
    "            # If I have reached at least 20% of the total number of batches or samples\n",
    "            # This allows to limit the memory occupancy of the sparse matrix\n",
    "            if self.train_with_sparse_weights and numCurrentBatch % (totalNumberOfBatch/5) == 0 and numCurrentBatch!=0:\n",
    "                self.S_sparse.rebalance_tree(TopK=self.topK)\n",
    "\n",
    "\n",
    "            if((numCurrentBatch%printStep==0 and not numCurrentBatch==0) or numCurrentBatch==totalNumberOfBatch-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. BPR loss is {:.2E}. Sample per second: {:.0f}\".format(\n",
    "                    numCurrentBatch*self.batch_size,\n",
    "                    100.0* float(numCurrentBatch*self.batch_size)/self.numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    loss/(numCurrentBatch*self.batch_size + 1),\n",
    "                    float(numCurrentBatch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_S(self):\n",
    "\n",
    "        # FIll diagonal with zeros\n",
    "        cdef int index = 0\n",
    "\n",
    "        while index < self.n_items:\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                self.S_sparse.add_value(index, index, -self.S_sparse.get_value(index, index))\n",
    "\n",
    "            elif self.symmetric:\n",
    "                self.S_symmetric.add_value(index, index, -self.S_symmetric.get_value(index, index))\n",
    "\n",
    "            else:\n",
    "                self.S_dense[index, index] = 0.0\n",
    "\n",
    "            index+=1\n",
    "\n",
    "\n",
    "\n",
    "        if self.topK == False:\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                return self.S_sparse.get_scipy_csr(TopK = False)\n",
    "\n",
    "            elif self.symmetric:\n",
    "                return self.S_symmetric.get_scipy_csr(TopK = False)\n",
    "\n",
    "            else:\n",
    "\n",
    "                if self.final_model_sparse_weights:\n",
    "                    return similarityMatrixTopK(np.array(self.S_dense.T), k=self.topK, forceSparseOutput=True, inplace=True).T\n",
    "                else:\n",
    "                    return np.array(self.S_dense)\n",
    "\n",
    "\n",
    "        else :\n",
    "\n",
    "            if self.train_with_sparse_weights:\n",
    "                return self.S_sparse.get_scipy_csr(TopK=self.topK)\n",
    "\n",
    "            elif self.symmetric:\n",
    "                return self.S_symmetric.get_scipy_csr(TopK=self.topK)\n",
    "\n",
    "            else:\n",
    "                if self.final_model_sparse_weights:\n",
    "                    return similarityMatrixTopK(np.array(self.S_dense.T), k=self.topK, forceSparseOutput=True, inplace=True).T\n",
    "                else:\n",
    "                    return np.array(self.S_dense)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef BPR_sample sampleBPR_Cython(self):\n",
    "\n",
    "        cdef BPR_sample sample = BPR_sample(-1,-1,-1,-1,-1)\n",
    "\n",
    "        cdef long index\n",
    "\n",
    "        cdef int negItemSelected, numSeenItems = 0\n",
    "\n",
    "\n",
    "        # Skip users with no interactions or with no negative items\n",
    "        while numSeenItems == 0 or numSeenItems == self.n_items:\n",
    "\n",
    "            sample.user = rand() % self.n_users\n",
    "\n",
    "            sample.seen_items_start_pos = self.URM_mask_indptr[sample.user]\n",
    "            sample.seen_items_end_pos = self.URM_mask_indptr[sample.user + 1]\n",
    "\n",
    "            numSeenItems = sample.seen_items_end_pos - sample.seen_items_start_pos\n",
    "\n",
    "\n",
    "        index = rand() % numSeenItems\n",
    "\n",
    "        sample.pos_item = self.URM_mask_indices[sample.seen_items_start_pos + index]\n",
    "\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        # for every user\n",
    "        while (not negItemSelected):\n",
    "\n",
    "            sample.neg_item = rand() % self.n_items\n",
    "\n",
    "            index = 0\n",
    "            while index < numSeenItems and self.URM_mask_indices[sample.seen_items_start_pos + index]!=sample.neg_item:\n",
    "                index+=1\n",
    "\n",
    "            if index == numSeenItems:\n",
    "                negItemSelected = True\n",
    "\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            SPARSE MATRIX\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "#from libc.stdlib cimport malloc, free#, qsort\n",
    "# PyMem malloc and free are slightly faster than plain C equivalents as they optimize OS calls\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "# Declaring QSORT as \"gil safe\", appending \"nogil\" at the end of the declaration\n",
    "# Otherwise I will not be able to pass the comparator function pointer\n",
    "# https://stackoverflow.com/questions/8353076/how-do-i-pass-a-pointer-to-a-c-function-in-cython\n",
    "cdef extern from \"stdlib.h\":\n",
    "    ctypedef void const_void \"const void\"\n",
    "    void qsort(void *base, int nmemb, int size,\n",
    "            int(*compar)(const_void *, const_void *)) nogil\n",
    "\n",
    "\n",
    "# Node struct\n",
    "ctypedef struct matrix_element_tree_s:\n",
    "    long column\n",
    "    double data\n",
    "    matrix_element_tree_s *higher\n",
    "    matrix_element_tree_s *lower\n",
    "\n",
    "ctypedef struct head_pointer_tree_s:\n",
    "    matrix_element_tree_s *head\n",
    "\n",
    "\n",
    "# Function to allocate a new node\n",
    "cdef matrix_element_tree_s * pointer_new_matrix_element_tree_s(long column, double data, matrix_element_tree_s *higher,  matrix_element_tree_s *lower):\n",
    "\n",
    "    cdef matrix_element_tree_s * new_element\n",
    "\n",
    "    new_element = < matrix_element_tree_s * > PyMem_Malloc(sizeof(matrix_element_tree_s))\n",
    "    new_element.column = column\n",
    "    new_element.data = data\n",
    "    new_element.higher = higher\n",
    "    new_element.lower = lower\n",
    "\n",
    "    return new_element\n",
    "\n",
    "\n",
    "# Functions to compare structs to be used in C qsort\n",
    "cdef int compare_struct_on_column(const void *a_input, const void *b_input):\n",
    "    \"\"\"\n",
    "    The function compares the column contained in the two struct passed.\n",
    "    If a.column > b.column returns >0  \n",
    "    If a.column < b.column returns <0      \n",
    "    \n",
    "    :return int: a.column - b.column\n",
    "    \"\"\"\n",
    "\n",
    "    cdef head_pointer_tree_s *a_casted = <head_pointer_tree_s *> a_input\n",
    "    cdef head_pointer_tree_s *b_casted = <head_pointer_tree_s *> b_input\n",
    "\n",
    "    return a_casted.head.column  - b_casted.head.column\n",
    "\n",
    "\n",
    "\n",
    "cdef int compare_struct_on_data(const void * a_input, const void * b_input):\n",
    "    \"\"\"\n",
    "    The function compares the data contained in the two struct passed.\n",
    "    If a.data > b.data returns >0  \n",
    "    If a.data < b.data returns <0      \n",
    "    \n",
    "    :return int: +1 or -1\n",
    "    \"\"\"\n",
    "\n",
    "    cdef head_pointer_tree_s * a_casted = <head_pointer_tree_s *> a_input\n",
    "    cdef head_pointer_tree_s * b_casted = <head_pointer_tree_s *> b_input\n",
    "\n",
    "    if (a_casted.head.data - b_casted.head.data) > 0.0:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "#################################\n",
    "#################################       CLASS DECLARATION\n",
    "#################################\n",
    "\n",
    "cdef class Sparse_Matrix_Tree_CSR:\n",
    "\n",
    "    cdef long num_rows, num_cols\n",
    "\n",
    "    # Array containing the struct (object, not pointer) corresponding to the root of the tree\n",
    "    cdef head_pointer_tree_s* row_pointer\n",
    "\n",
    "\n",
    "    def __init__(self, long num_rows, long num_cols):\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "        self.row_pointer = < head_pointer_tree_s *> PyMem_Malloc(self.num_rows * sizeof(head_pointer_tree_s))\n",
    "\n",
    "        # Initialize all rows to empty\n",
    "        for index in range(self.num_rows):\n",
    "            self.row_pointer[index].head = NULL\n",
    "\n",
    "\n",
    "\n",
    "    def dealloc(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int numRow\n",
    "\n",
    "        # Free all rows memory\n",
    "        for numRow in range(self.num_rows):\n",
    "            self.subtree_free_memory(self.row_pointer[numRow].head)\n",
    "\n",
    "        PyMem_Free(self.row_pointer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double add_value(self, long row, long col, double value):\n",
    "        \"\"\"\n",
    "        The function adds a value to the specified cell. A new cell is created if necessary.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :param value: value to add\n",
    "        :return double: resulting cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        cdef matrix_element_tree_s* current_element, new_element, * old_element\n",
    "        cdef int stopSearch = False\n",
    "\n",
    "\n",
    "        # If the row is empty, create a new element\n",
    "        if self.row_pointer[row].head == NULL:\n",
    "\n",
    "            # row_pointer is a python object, so I need the object itself and not the address\n",
    "            self.row_pointer[row].head = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "\n",
    "        # If the row is not empty, look for the cell\n",
    "        # row_pointer contains the struct itself, but I just want its address\n",
    "        current_element = self.row_pointer[row].head\n",
    "\n",
    "        # Follow the tree structure\n",
    "        while not stopSearch:\n",
    "\n",
    "            if current_element.column < col and current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            elif current_element.column > col and current_element.lower != NULL:\n",
    "                current_element = current_element.lower\n",
    "\n",
    "            else:\n",
    "                stopSearch = True\n",
    "\n",
    "        # If the cell exist, update its value\n",
    "        if current_element.column == col:\n",
    "            current_element.data += value\n",
    "\n",
    "            return current_element.data\n",
    "\n",
    "\n",
    "        # The cell is not found, create new Higher element\n",
    "        elif current_element.column < col and current_element.higher == NULL:\n",
    "\n",
    "            current_element.higher = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "        # The cell is not found, create new Lower element\n",
    "        elif current_element.column > col and current_element.lower == NULL:\n",
    "\n",
    "            current_element.lower = pointer_new_matrix_element_tree_s(col, value, NULL, NULL)\n",
    "\n",
    "            return value\n",
    "\n",
    "        else:\n",
    "            assert False, 'ERROR - Current insert operation is not implemented'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double get_value(self, long row, long col):\n",
    "        \"\"\"\n",
    "        The function returns the value of the specified cell.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :return double: cell value\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\n",
    "                \"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                    self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "\n",
    "        cdef matrix_element_tree_s* current_element\n",
    "        cdef int stopSearch = False\n",
    "\n",
    "        # If the row is empty, return default\n",
    "        if self.row_pointer[row].head == NULL:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "        # If the row is not empty, look for the cell\n",
    "        # row_pointer contains the struct itself, but I just want its address\n",
    "        current_element = self.row_pointer[row].head\n",
    "\n",
    "        # Follow the tree structure\n",
    "        while not stopSearch:\n",
    "\n",
    "            if current_element.column < col and current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            elif current_element.column > col and current_element.lower != NULL:\n",
    "                current_element = current_element.lower\n",
    "\n",
    "            else:\n",
    "                stopSearch = True\n",
    "\n",
    "\n",
    "        # If the cell exist, return its value\n",
    "        if current_element.column == col:\n",
    "            return current_element.data\n",
    "\n",
    "        # The cell is not found, return default\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef get_scipy_csr(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function returns the current sparse matrix as a scipy_csr object         \n",
    "   \n",
    "        :return double: scipy_csr object\n",
    "        \"\"\"\n",
    "        cdef int terminate\n",
    "        cdef long row\n",
    "\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        # Loop the rows\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            #Always set indptr\n",
    "            indptr.append(len(data))\n",
    "\n",
    "            # row contains data\n",
    "            if self.row_pointer[row].head != NULL:\n",
    "\n",
    "                # Flatten the data structure\n",
    "                self.row_pointer[row].head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"subtree_to_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                if TopK:\n",
    "                    self.row_pointer[row].head = self.topK_selection_from_list(self.row_pointer[row].head, TopK)\n",
    "                    #print(\"topK_selection_from_list {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "                # Flatten the tree data\n",
    "                subtree_column, subtree_data = self.from_linked_list_to_python_list(self.row_pointer[row].head)\n",
    "                data.extend(subtree_data)\n",
    "                indices.extend(subtree_column)\n",
    "\n",
    "                # Rebuild the tree\n",
    "                self.row_pointer[row].head = self.build_tree_from_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"build_tree_from_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "        #Set terminal indptr\n",
    "        indptr.append(len(data))\n",
    "\n",
    "        return sps.csr_matrix((data, indices, indptr), shape=(self.num_rows, self.num_cols))\n",
    "\n",
    "\n",
    "\n",
    "    cpdef rebalance_tree(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function builds a balanced binary tree from the current one, for all matrix rows\n",
    "        \n",
    "        :param TopK: either False or an integer number. Number of the highest elements to preserve\n",
    "        \"\"\"\n",
    "\n",
    "        cdef long row\n",
    "\n",
    "        #start_time = time.time()\n",
    "\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            if self.row_pointer[row].head != NULL:\n",
    "\n",
    "                # Flatten the data structure\n",
    "                self.row_pointer[row].head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"subtree_to_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                if TopK:\n",
    "                    self.row_pointer[row].head = self.topK_selection_from_list(self.row_pointer[row].head, TopK)\n",
    "                    #print(\"topK_selection_from_list {} sec\".format(time.time() - start_time))\n",
    "\n",
    "                # Rebuild the tree\n",
    "                self.row_pointer[row].head = self.build_tree_from_list_flat(self.row_pointer[row].head)\n",
    "                #print(\"build_tree_from_list_flat {} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s * subtree_to_list_flat(self, matrix_element_tree_s * root):\n",
    "        \"\"\"\n",
    "        The function flatten the structure of the subtree whose root is passed as a paramether    \n",
    "        The list is bidirectional and ordered with respect to the column\n",
    "        The column ordering follows from the insertion policy\n",
    "        \n",
    "        :param root: tree root\n",
    "        :return list, list: data and corresponding column. Empty list if root is None\n",
    "        \"\"\"\n",
    "\n",
    "        if root == NULL:\n",
    "            return NULL\n",
    "\n",
    "        cdef matrix_element_tree_s *flat_list_head, *current_element\n",
    "\n",
    "        # Flatten lower subtree\n",
    "        flat_list_head = self.subtree_to_list_flat(root.lower)\n",
    "\n",
    "        # If no lower elements exist, the head is the current element\n",
    "        if flat_list_head == NULL:\n",
    "            flat_list_head = root\n",
    "            root.lower = NULL\n",
    "\n",
    "        # Else move to the tail and add the subtree root\n",
    "        else:\n",
    "            current_element = flat_list_head\n",
    "            while current_element.higher != NULL:\n",
    "                current_element = current_element.higher\n",
    "\n",
    "            # Attach the element with the bidirectional pointers\n",
    "            current_element.higher = root\n",
    "            root.lower = current_element\n",
    "\n",
    "        # Flatten higher subtree and attach it to the tail of the flat list\n",
    "        root.higher = self.subtree_to_list_flat(root.higher)\n",
    "\n",
    "        # Attach the element with the bidirectional pointers\n",
    "        if root.higher != NULL:\n",
    "            root.higher.lower = root\n",
    "\n",
    "        return flat_list_head\n",
    "\n",
    "\n",
    "\n",
    "    cdef from_linked_list_to_python_list(self, matrix_element_tree_s * head):\n",
    "\n",
    "        data = []\n",
    "        column = []\n",
    "\n",
    "        while head != NULL:\n",
    "\n",
    "            if head.data != 0.0:\n",
    "                data.append(head.data)\n",
    "                column.append(head.column)\n",
    "\n",
    "            head = head.higher\n",
    "\n",
    "        return column, data\n",
    "\n",
    "\n",
    "\n",
    "    cdef subtree_free_memory(self, matrix_element_tree_s* root):\n",
    "        \"\"\"\n",
    "        The function frees all struct in the subtree whose root is passed as a parameter, root included \n",
    "        \n",
    "        :param root: tree root\n",
    "        \"\"\"\n",
    "\n",
    "        if root != NULL:\n",
    "            # If the root exists, open recursion\n",
    "            self.subtree_free_memory(root.higher)\n",
    "            self.subtree_free_memory(root.lower)\n",
    "\n",
    "            # Once the lower elements have been reached, start freeing from the bottom\n",
    "            PyMem_Free(root)\n",
    "\n",
    "\n",
    "\n",
    "    cdef list_free_memory(self, matrix_element_tree_s * head):\n",
    "        \"\"\"\n",
    "        The function frees all struct in the list whose head is passed as a parameter, head included \n",
    "        \n",
    "        :param head: list head\n",
    "        \"\"\"\n",
    "\n",
    "        if head != NULL:\n",
    "            # If the root exists, open recursion\n",
    "            self.subtree_free_memory(head.higher)\n",
    "\n",
    "            # Once the tail element have been reached, start freeing from them\n",
    "            PyMem_Free(head)\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s* build_tree_from_list_flat(self, matrix_element_tree_s* flat_list_head):\n",
    "        \"\"\"\n",
    "        The function builds a tree containing the passed data. This is the recursive function, the \n",
    "        data should be sorted by te caller\n",
    "        To ensure the tree is balanced, data is sorted according to the column   \n",
    "        \n",
    "        :param row: row in which to create new tree\n",
    "        :param column_vector: column coordinates \n",
    "        :param data_vector: cell data\n",
    "        \"\"\"\n",
    "\n",
    "        if flat_list_head == NULL:\n",
    "            return NULL\n",
    "\n",
    "\n",
    "        cdef long list_length = 0\n",
    "        cdef long middle_element_step = 0\n",
    "\n",
    "        cdef matrix_element_tree_s *current_element, *middleElement, *tree_root\n",
    "\n",
    "        current_element = flat_list_head\n",
    "        middleElement = flat_list_head\n",
    "\n",
    "        # Explore the flat list moving the middle elment every tho jumps\n",
    "        while current_element != NULL:\n",
    "            current_element = current_element.higher\n",
    "            list_length += 1\n",
    "            middle_element_step += 1\n",
    "\n",
    "            if middle_element_step == 2:\n",
    "                middleElement = middleElement.higher\n",
    "                middle_element_step = 0\n",
    "\n",
    "        tree_root = middleElement\n",
    "\n",
    "        # To execute the recursion it is necessary to cut the flat list\n",
    "        # The last of the lower elements will have to be a tail\n",
    "        if middleElement.lower != NULL:\n",
    "            middleElement.lower.higher = NULL\n",
    "\n",
    "            tree_root.lower = self.build_tree_from_list_flat(flat_list_head)\n",
    "\n",
    "\n",
    "        # The first of the higher elements will have to be a head\n",
    "        if middleElement.higher != NULL:\n",
    "            middleElement.higher.lower = NULL\n",
    "\n",
    "            tree_root.higher = self.build_tree_from_list_flat(middleElement.higher)\n",
    "\n",
    "\n",
    "        return tree_root\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cdef matrix_element_tree_s* topK_selection_from_list(self, matrix_element_tree_s* head, long TopK):\n",
    "        \"\"\"\n",
    "        The function selects the topK highest elements in the given list \n",
    "        \n",
    "        :param head: head of the list\n",
    "        :param TopK: number of highest elements to preserve\n",
    "        :return matrix_element_tree_s*: head of the new list\n",
    "        \"\"\"\n",
    "\n",
    "        cdef head_pointer_tree_s *vector_pointer_to_list_elements\n",
    "        cdef matrix_element_tree_s *current_element\n",
    "        cdef long list_length, index, selected_count\n",
    "\n",
    "        # Get list size\n",
    "        current_element = head\n",
    "        list_length = 0\n",
    "\n",
    "        while current_element != NULL:\n",
    "            list_length += 1\n",
    "            current_element = current_element.higher\n",
    "\n",
    "\n",
    "        # If list elements are not enough to perform a selection, return\n",
    "        if list_length < TopK:\n",
    "            return head\n",
    "\n",
    "        # Allocate vector that will be used for sorting\n",
    "        vector_pointer_to_list_elements = < head_pointer_tree_s *> PyMem_Malloc(list_length * sizeof(head_pointer_tree_s))\n",
    "\n",
    "        # Fill vector wit pointers to list elements\n",
    "        current_element = head\n",
    "        for index in range(list_length):\n",
    "            vector_pointer_to_list_elements[index].head = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "\n",
    "        # Sort array elements on their data field\n",
    "        qsort(vector_pointer_to_list_elements, list_length, sizeof(head_pointer_tree_s), compare_struct_on_data)\n",
    "\n",
    "        # Sort only the TopK according to their column field\n",
    "        # Sort is from lower to higher, therefore the elements to be considered are from len-topK to len\n",
    "        qsort(&vector_pointer_to_list_elements[list_length-TopK], TopK, sizeof(head_pointer_tree_s), compare_struct_on_column)\n",
    "\n",
    "\n",
    "        # Rebuild list attaching the consecutive elements\n",
    "        index = list_length-TopK\n",
    "\n",
    "        # Detach last TopK element from previous ones\n",
    "        vector_pointer_to_list_elements[index].head.lower = NULL\n",
    "\n",
    "        while index<list_length-1:\n",
    "            # Rearrange bidirectional pointers\n",
    "            vector_pointer_to_list_elements[index+1].head.lower = vector_pointer_to_list_elements[index].head\n",
    "            vector_pointer_to_list_elements[index].head.higher = vector_pointer_to_list_elements[index+1].head\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        # Last element in vector will be the hew head\n",
    "        vector_pointer_to_list_elements[list_length - 1].head.higher = NULL\n",
    "\n",
    "        # Get hew list head\n",
    "        current_element = vector_pointer_to_list_elements[list_length-TopK].head\n",
    "\n",
    "        # If there are exactly enough elements to reach TopK, index == 0 will be the tail\n",
    "        # Else, index will be the tail and the other elements will be removed\n",
    "        index = list_length - TopK - 1\n",
    "        if index > 0:\n",
    "\n",
    "            index -= 1\n",
    "            while index >= 0:\n",
    "                PyMem_Free(vector_pointer_to_list_elements[index].head)\n",
    "                index -= 1\n",
    "\n",
    "        # Free array\n",
    "        PyMem_Free(vector_pointer_to_list_elements)\n",
    "\n",
    "\n",
    "        return current_element\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            TEST FUNCTIONS\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "    cpdef test_list_tee_conversion(self, long row):\n",
    "        \"\"\"\n",
    "        The function tests the inner data structure conversion from tree to C linked list and back to tree\n",
    "        \n",
    "        :param row: row to use for testing\n",
    "        \"\"\"\n",
    "\n",
    "        cdef matrix_element_tree_s *head, *tree_root\n",
    "        cdef matrix_element_tree_s *current_element, *previous_element\n",
    "\n",
    "        head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "        current_element = head\n",
    "\n",
    "        cdef numElements_higher = 0\n",
    "        cdef numElements_lower = 0\n",
    "\n",
    "        while current_element != NULL:\n",
    "            numElements_higher += 1\n",
    "            previous_element = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "        current_element = previous_element\n",
    "        while current_element != NULL:\n",
    "            numElements_lower += 1\n",
    "            current_element = current_element.lower\n",
    "\n",
    "        assert numElements_higher == numElements_lower, 'Bidirectional linked list not consistent.' \\\n",
    "                                                        ' From head to tail element count is {}, from tail to head is {}'.format(\n",
    "                                                        numElements_higher, numElements_lower)\n",
    "\n",
    "        print(\"Bidirectional list link - Passed\")\n",
    "\n",
    "        column_original, data_original = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert numElements_higher == len(column_original), \\\n",
    "            'Data structure size inconsistent. LinkedList is {}, Python list is {}'.format(numElements_higher, len(column_original))\n",
    "\n",
    "        for index in range(len(column_original)-1):\n",
    "            assert column_original[index] < column_original[index+1],\\\n",
    "                'Columns not ordered correctly. Tree not flattened properly'\n",
    "\n",
    "        print(\"Bidirectional list ordering - Passed\")\n",
    "\n",
    "        # Transform list into tree and back into list, as it is easy to test\n",
    "        tree_root = self.build_tree_from_list_flat(head)\n",
    "        head = self.subtree_to_list_flat(tree_root)\n",
    "\n",
    "        cdef numElements_higher_after = 0\n",
    "        cdef numElements_lower_after = 0\n",
    "\n",
    "        current_element = head\n",
    "\n",
    "        while current_element != NULL:\n",
    "            numElements_higher_after += 1\n",
    "            previous_element = current_element\n",
    "            current_element = current_element.higher\n",
    "\n",
    "        current_element = previous_element\n",
    "        while current_element != NULL:\n",
    "            numElements_lower_after += 1\n",
    "            current_element = current_element.lower\n",
    "\n",
    "        print(\"Bidirectional list from tree link - Passed\")\n",
    "\n",
    "        assert numElements_higher_after == numElements_lower_after, \\\n",
    "            'Bidirectional linked list after tree construction not consistent. ' \\\n",
    "            'From head to tail element count is {}, from tail to head is {}'.format(\n",
    "            numElements_higher_after, numElements_lower_after)\n",
    "\n",
    "        assert numElements_higher == numElements_higher_after, \\\n",
    "            'Data structure size inconsistent. Original length is {}, after tree conversion is {}'.format(\n",
    "                numElements_higher, numElements_higher_after)\n",
    "\n",
    "        column_after_tree, data_after_tree = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert len(column_original) == len(column_after_tree), \\\n",
    "            'Data structure size inconsistent. Original length is {}, after tree conversion is {}'.format(\n",
    "                len(column_original), len(column_after_tree))\n",
    "\n",
    "        for index in range(len(column_original)):\n",
    "            assert column_original[index] == column_after_tree[index],\\\n",
    "                'After tree construction columns are not ordered properly'\n",
    "            assert data_original[index] == data_after_tree[index],\\\n",
    "                'After tree construction data content is changed'\n",
    "\n",
    "        print(\"Bidirectional list from tree ordering - Passed\")\n",
    "\n",
    "\n",
    "\n",
    "    cpdef test_topK_from_list_selection(self, long row, long topK):\n",
    "        \"\"\"\n",
    "        The function tests the topK selection from list\n",
    "        \n",
    "        :param row: row to use for testing\n",
    "        \"\"\"\n",
    "\n",
    "        cdef matrix_element_tree_s *head\n",
    "\n",
    "        head = self.subtree_to_list_flat(self.row_pointer[row].head)\n",
    "\n",
    "        column_original, data_original = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        head = self.topK_selection_from_list(head, topK)\n",
    "\n",
    "        column_topK, data_topK = self.from_linked_list_to_python_list(head)\n",
    "\n",
    "        assert len(column_topK) == len(data_topK),\\\n",
    "            \"TopK data and column lists have different length. Columns length is {}, data is {}\".format(len(column_topK), len(data_topK))\n",
    "        assert len(column_topK) <= topK,\\\n",
    "            \"TopK extracted list is longer than desired value. Desired is {}, while list is {}\".format(topK, len(column_topK))\n",
    "\n",
    "        print(\"TopK extracted length - Passed\")\n",
    "\n",
    "        # Sort with respect to the content to select topK\n",
    "        idx_sorted = np.argsort(data_original)\n",
    "        idx_sorted = np.flip(idx_sorted, axis=0)\n",
    "        top_k_idx = idx_sorted[0:topK]\n",
    "\n",
    "        column_topK_numpy = np.array(column_original)[top_k_idx]\n",
    "        data_topK_numpy = np.array(data_original)[top_k_idx]\n",
    "\n",
    "        # Sort with respect to the column to ensure it is ordered as the tree flattened list\n",
    "        idx_sorted = np.argsort(column_topK_numpy)\n",
    "        column_topK_numpy = column_topK_numpy[idx_sorted]\n",
    "        data_topK_numpy = data_topK_numpy[idx_sorted]\n",
    "\n",
    "\n",
    "        assert len(column_topK_numpy) <= len(column_topK),\\\n",
    "            \"TopK extracted list and numpy one have different length. Extracted list lenght is {}, while numpy is {}\".format(\n",
    "                len(column_topK_numpy), len(column_topK))\n",
    "\n",
    "\n",
    "        for index in range(len(column_topK)):\n",
    "\n",
    "            assert column_topK[index] == column_topK_numpy[index], \\\n",
    "                \"TopK extracted list and numpy one have different content at index {} as column value.\" \\\n",
    "                \" Extracted list lenght is {}, while numpy is {}\".format(index, column_topK[index], column_topK_numpy[index])\n",
    "\n",
    "            assert data_topK[index] == data_topK_numpy[index], \\\n",
    "                \"TopK extracted list and numpy one have different content at index {} as data value.\" \\\n",
    "                \" Extracted list lenght is {}, while numpy is {}\".format(index, data_topK[index], data_topK_numpy[index])\n",
    "\n",
    "        print(\"TopK extracted content - Passed\")\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "#####################\n",
    "#####################            TRIANGULAR MATRIX\n",
    "#####################\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "#from libc.stdlib cimport malloc\n",
    "# PyMem malloc and free are slightly faster than plain C equivalents as they optimize OS calls\n",
    "from cpython.mem cimport PyMem_Malloc, PyMem_Free\n",
    "\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "\n",
    "#################################\n",
    "#################################       CLASS DECLARATION\n",
    "#################################\n",
    "\n",
    "cdef class Triangular_Matrix:\n",
    "\n",
    "    cdef long num_rows, num_cols\n",
    "    cdef int isSymmetric\n",
    "\n",
    "    cdef double** row_pointer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, long num_rows, int isSymmetric = False):\n",
    "\n",
    "        cdef int numRow, numCol\n",
    "\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_rows\n",
    "        self.isSymmetric = isSymmetric\n",
    "\n",
    "        self.row_pointer = <double **> PyMem_Malloc(self.num_rows * sizeof(double*))\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize all rows to empty\n",
    "        for numRow in range(self.num_rows):\n",
    "            self.row_pointer[numRow] = < double *> PyMem_Malloc((numRow+1) * sizeof(double))\n",
    "\n",
    "            for numCol in range(numRow+1):\n",
    "                self.row_pointer[numRow][numCol] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    def dealloc(self):\n",
    "        \"\"\"\n",
    "        Remove all PyMalloc allocaded memory\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int numRow\n",
    "\n",
    "        # Free all rows memory\n",
    "        for numRow in range(self.num_rows):\n",
    "            PyMem_Free(self.row_pointer[numRow])\n",
    "\n",
    "        PyMem_Free(self.row_pointer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double add_value(self, long row, long col, double value):\n",
    "        \"\"\"\n",
    "        The function adds a value to the specified cell. A new cell is created if necessary.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :param value: value to add\n",
    "        :return double: resulting cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}),\"\n",
    "                             \" coordinates given are ({},{})\".format(\n",
    "                             self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        elif col > row:\n",
    "\n",
    "            if self.isSymmetric:\n",
    "                self.row_pointer[col][row] += value\n",
    "\n",
    "                return self.row_pointer[col][row]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Cell is in the upper triangular of the matrix,\"\n",
    "                                 \" current matrix is lower triangular.\"\n",
    "                                 \" Coordinates given are ({},{})\".format(row, col))\n",
    "        else:\n",
    "\n",
    "            self.row_pointer[row][col] += value\n",
    "\n",
    "            return self.row_pointer[row][col]\n",
    "\n",
    "\n",
    "\n",
    "    cpdef double get_value(self, long row, long col):\n",
    "        \"\"\"\n",
    "        The function returns the value of the specified cell.         \n",
    "        \n",
    "        :param row: cell coordinates\n",
    "        :param col:  cell coordinates\n",
    "        :return double: cell value\n",
    "        \"\"\"\n",
    "\n",
    "        if row >= self.num_rows or col >= self.num_cols or row < 0 or col < 0:\n",
    "            raise ValueError(\"Cell is outside matrix. Matrix shape is ({},{}), coordinates given are ({},{})\".format(\n",
    "                self.num_rows, self.num_cols, row, col))\n",
    "\n",
    "        elif col > row:\n",
    "\n",
    "            if self.isSymmetric:\n",
    "                return self.row_pointer[col][row]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Cell is in the upper triangular of the matrix,\"\n",
    "                                 \" current matrix is lower triangular.\"\n",
    "                                 \" Coordinates given are ({},{})\".format(row, col))\n",
    "        else:\n",
    "\n",
    "            return self.row_pointer[row][col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cpdef get_scipy_csr(self, long TopK = False):\n",
    "        \"\"\"\n",
    "        The function returns the current sparse matrix as a scipy_csr object         \n",
    "   \n",
    "        :return double: scipy_csr object\n",
    "        \"\"\"\n",
    "        cdef int terminate\n",
    "        cdef long row, col, index\n",
    "\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] currentRowArray = clone(template_zero, self.num_cols, zero=True)\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] currentRowArray_np\n",
    "\n",
    "\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        # Loop the rows\n",
    "        for row in range(self.num_rows):\n",
    "\n",
    "            #Always set indptr\n",
    "            indptr.append(len(data))\n",
    "\n",
    "            # Fill RowArray\n",
    "            for col in range(self.num_cols):\n",
    "\n",
    "                if col <= row:\n",
    "                    currentRowArray[col] = self.row_pointer[row][col]\n",
    "                else:\n",
    "                    if self.isSymmetric:\n",
    "                        currentRowArray[col] = self.row_pointer[col][row]\n",
    "                    else:\n",
    "                        currentRowArray[col] = 0.0\n",
    "\n",
    "\n",
    "            if TopK:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                currentRowArray_np = - np.array(currentRowArray)\n",
    "                #\n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(currentRowArray_np, TopK-1)[0:TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(currentRowArray_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "                for index in range(len(top_k_idx)):\n",
    "\n",
    "                    col = top_k_idx[index]\n",
    "\n",
    "                    if currentRowArray[col] != 0.0:\n",
    "                        indices.append(col)\n",
    "                        data.append(currentRowArray[col])\n",
    "\n",
    "            else:\n",
    "\n",
    "                for index in range(self.num_cols):\n",
    "\n",
    "                    if currentRowArray[index] != 0.0:\n",
    "                        indices.append(index)\n",
    "                        data.append(currentRowArray[index])\n",
    "\n",
    "\n",
    "        #Set terminal indptr\n",
    "        indptr.append(len(data))\n",
    "\n",
    "        return sps.csr_matrix((data, indices, indptr), shape=(self.num_rows, self.num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75dfabeb61b21353005a0861ec879419e2eb5d24"
   },
   "source": [
    "# Training of the SLIM BPR Cython models for UCF, ICF, ICBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "80f6c6657888255e1c2dc1ae1cc14bbfff80ced4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model - Creation\n",
      "SLIM_BPR_Cython: Estimated memory required for similarity matrix of 20635 items is 1703.21 MB\n",
      "Model - Fitting\n",
      "Similarity column 50446 ( 100 % ), 2179.13 column/sec, elapsed time 0.39 min\n",
      "Similarity column 20635 ( 100 % ), 5354.94 column/sec, elapsed time 0.06 min\n",
      "Similarity column 20635 ( 100 % ), 5832.57 column/sec, elapsed time 0.06 min\n",
      "Processed 806421 ( 100.00% ) in 3.35 seconds. BPR loss is 2.00E+00. Sample per second: 240954\n",
      "SLIM_BPR_Recommender + : Epoch 1 of 40. Elapsed time 0.18 min\n",
      "Processed 806421 ( 100.00% ) in 2.61 seconds. BPR loss is 4.11E+00. Sample per second: 308724\n",
      "SLIM_BPR_Recommender + : Epoch 2 of 40. Elapsed time 0.22 min\n",
      "Processed 806421 ( 100.00% ) in 2.50 seconds. BPR loss is 5.45E+00. Sample per second: 322531\n",
      "SLIM_BPR_Recommender + : Epoch 3 of 40. Elapsed time 0.25 min\n",
      "Processed 806421 ( 100.00% ) in 2.65 seconds. BPR loss is 6.49E+00. Sample per second: 304254\n",
      "SLIM_BPR_Recommender + : Epoch 4 of 40. Elapsed time 0.29 min\n",
      "Processed 806421 ( 100.00% ) in 2.93 seconds. BPR loss is 7.32E+00. Sample per second: 275483\n",
      "SLIM_BPR_Recommender + : Epoch 5 of 40. Elapsed time 0.33 min\n",
      "Processed 806421 ( 100.00% ) in 3.00 seconds. BPR loss is 8.03E+00. Sample per second: 268947\n",
      "SLIM_BPR_Recommender + : Epoch 6 of 40. Elapsed time 0.36 min\n",
      "Processed 806421 ( 100.00% ) in 3.01 seconds. BPR loss is 8.60E+00. Sample per second: 267941\n",
      "SLIM_BPR_Recommender + : Epoch 7 of 40. Elapsed time 0.39 min\n",
      "Processed 806421 ( 100.00% ) in 2.03 seconds. BPR loss is 9.14E+00. Sample per second: 397715\n",
      "SLIM_BPR_Recommender + : Epoch 8 of 40. Elapsed time 0.43 min\n",
      "Processed 806421 ( 100.00% ) in 2.19 seconds. BPR loss is 9.62E+00. Sample per second: 368307\n",
      "SLIM_BPR_Recommender + : Epoch 9 of 40. Elapsed time 0.46 min\n",
      "Processed 806421 ( 100.00% ) in 2.76 seconds. BPR loss is 1.01E+01. Sample per second: 291884\n",
      "SLIM_BPR_Recommender + : Epoch 10 of 40. Elapsed time 0.51 min\n",
      "Processed 806421 ( 100.00% ) in 3.21 seconds. BPR loss is 1.04E+01. Sample per second: 251013\n",
      "SLIM_BPR_Recommender + : Epoch 11 of 40. Elapsed time 0.55 min\n",
      "Processed 806421 ( 100.00% ) in 2.58 seconds. BPR loss is 1.08E+01. Sample per second: 312006\n",
      "SLIM_BPR_Recommender + : Epoch 12 of 40. Elapsed time 0.59 min\n",
      "Processed 806421 ( 100.00% ) in 2.90 seconds. BPR loss is 1.11E+01. Sample per second: 278264\n",
      "SLIM_BPR_Recommender + : Epoch 13 of 40. Elapsed time 0.63 min\n",
      "Processed 806421 ( 100.00% ) in 2.91 seconds. BPR loss is 1.14E+01. Sample per second: 277393\n",
      "SLIM_BPR_Recommender + : Epoch 14 of 40. Elapsed time 0.66 min\n",
      "Processed 806421 ( 100.00% ) in 2.53 seconds. BPR loss is 1.17E+01. Sample per second: 319136\n",
      "SLIM_BPR_Recommender + : Epoch 15 of 40. Elapsed time 0.69 min\n",
      "Processed 806421 ( 100.00% ) in 2.17 seconds. BPR loss is 1.19E+01. Sample per second: 371579\n",
      "SLIM_BPR_Recommender + : Epoch 16 of 40. Elapsed time 0.71 min\n",
      "Processed 806421 ( 100.00% ) in 2.17 seconds. BPR loss is 1.21E+01. Sample per second: 370986\n",
      "SLIM_BPR_Recommender + : Epoch 17 of 40. Elapsed time 0.75 min\n",
      "Processed 806421 ( 100.00% ) in 2.15 seconds. BPR loss is 1.24E+01. Sample per second: 375114\n",
      "SLIM_BPR_Recommender + : Epoch 18 of 40. Elapsed time 0.78 min\n",
      "Processed 806421 ( 100.00% ) in 1.79 seconds. BPR loss is 1.26E+01. Sample per second: 450918\n",
      "SLIM_BPR_Recommender + : Epoch 19 of 40. Elapsed time 0.81 min\n",
      "Processed 806421 ( 100.00% ) in 2.45 seconds. BPR loss is 1.28E+01. Sample per second: 329479\n",
      "SLIM_BPR_Recommender + : Epoch 20 of 40. Elapsed time 0.83 min\n",
      "Processed 806421 ( 100.00% ) in 2.12 seconds. BPR loss is 1.29E+01. Sample per second: 379807\n",
      "SLIM_BPR_Recommender + : Epoch 21 of 40. Elapsed time 0.86 min\n",
      "Processed 806421 ( 100.00% ) in 2.16 seconds. BPR loss is 1.31E+01. Sample per second: 373002\n",
      "SLIM_BPR_Recommender + : Epoch 22 of 40. Elapsed time 0.90 min\n",
      "Processed 806421 ( 100.00% ) in 2.14 seconds. BPR loss is 1.33E+01. Sample per second: 376960\n",
      "SLIM_BPR_Recommender + : Epoch 23 of 40. Elapsed time 0.93 min\n",
      "Processed 806421 ( 100.00% ) in 2.07 seconds. BPR loss is 1.34E+01. Sample per second: 390186\n",
      "SLIM_BPR_Recommender + : Epoch 24 of 40. Elapsed time 0.96 min\n",
      "Processed 806421 ( 100.00% ) in 1.93 seconds. BPR loss is 1.35E+01. Sample per second: 418190\n",
      "SLIM_BPR_Recommender + : Epoch 25 of 40. Elapsed time 0.99 min\n",
      "Processed 806421 ( 100.00% ) in 2.65 seconds. BPR loss is 1.37E+01. Sample per second: 303866\n",
      "SLIM_BPR_Recommender + : Epoch 26 of 40. Elapsed time 1.02 min\n",
      "Processed 806421 ( 100.00% ) in 2.46 seconds. BPR loss is 1.38E+01. Sample per second: 328159\n",
      "SLIM_BPR_Recommender + : Epoch 27 of 40. Elapsed time 1.05 min\n",
      "Processed 806421 ( 100.00% ) in 2.24 seconds. BPR loss is 1.40E+01. Sample per second: 360362\n",
      "SLIM_BPR_Recommender + : Epoch 28 of 40. Elapsed time 1.08 min\n",
      "Processed 806421 ( 100.00% ) in 2.30 seconds. BPR loss is 1.41E+01. Sample per second: 351089\n",
      "SLIM_BPR_Recommender + : Epoch 29 of 40. Elapsed time 1.12 min\n",
      "Processed 806421 ( 100.00% ) in 2.57 seconds. BPR loss is 1.42E+01. Sample per second: 313694\n",
      "SLIM_BPR_Recommender + : Epoch 30 of 40. Elapsed time 1.15 min\n",
      "Processed 806421 ( 100.00% ) in 2.85 seconds. BPR loss is 1.43E+01. Sample per second: 283433\n",
      "SLIM_BPR_Recommender + : Epoch 31 of 40. Elapsed time 1.19 min\n",
      "Processed 806421 ( 100.00% ) in 2.71 seconds. BPR loss is 1.44E+01. Sample per second: 298111\n",
      "SLIM_BPR_Recommender + : Epoch 32 of 40. Elapsed time 1.22 min\n",
      "Processed 806421 ( 100.00% ) in 2.75 seconds. BPR loss is 1.45E+01. Sample per second: 292894\n",
      "SLIM_BPR_Recommender + : Epoch 33 of 40. Elapsed time 1.26 min\n",
      "Processed 806421 ( 100.00% ) in 2.85 seconds. BPR loss is 1.46E+01. Sample per second: 283100\n",
      "SLIM_BPR_Recommender + : Epoch 34 of 40. Elapsed time 1.29 min\n",
      "Processed 806421 ( 100.00% ) in 3.16 seconds. BPR loss is 1.47E+01. Sample per second: 255248\n",
      "SLIM_BPR_Recommender + : Epoch 35 of 40. Elapsed time 1.33 min\n",
      "Processed 806421 ( 100.00% ) in 2.18 seconds. BPR loss is 1.47E+01. Sample per second: 369574\n",
      "SLIM_BPR_Recommender + : Epoch 36 of 40. Elapsed time 1.36 min\n",
      "Processed 806421 ( 100.00% ) in 1.82 seconds. BPR loss is 1.48E+01. Sample per second: 442977\n",
      "SLIM_BPR_Recommender + : Epoch 37 of 40. Elapsed time 1.39 min\n",
      "Processed 806421 ( 100.00% ) in 2.46 seconds. BPR loss is 1.49E+01. Sample per second: 328044\n",
      "SLIM_BPR_Recommender + : Epoch 38 of 40. Elapsed time 1.42 min\n",
      "Processed 806421 ( 100.00% ) in 2.55 seconds. BPR loss is 1.50E+01. Sample per second: 315725\n",
      "SLIM_BPR_Recommender + : Epoch 39 of 40. Elapsed time 1.45 min\n",
      "Processed 806421 ( 100.00% ) in 2.24 seconds. BPR loss is 1.50E+01. Sample per second: 359993\n",
      "SLIM_BPR_Recommender + : Epoch 40 of 40. Elapsed time 1.48 min\n",
      "Model - Fitting completed, ready to use\n"
     ]
    }
   ],
   "source": [
    "# MARK: - Train  algorithm\n",
    "\n",
    "print(\"Model - Creation\")\n",
    "slimbprrecommender = SLIM_BPR_Cython(URM_train, ICM, positive_threshold=0)\n",
    "print(\"Model - Fitting\")\n",
    "slimbprrecommender.fit(epochs=35, lambda_i=0.0025, lambda_j=0.0025, learning_rate=0.1)\n",
    "print(\"Model - Fitting completed, ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6364c7b5654565e306f93ac7b83266f418d0cbba"
   },
   "source": [
    "# Evaluation of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "5d2c9e99d9a2dc00002e28cd346b609912b1b31e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring 0 Items\n",
      "Ignoring 0 Users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:180: DeprecationWarning: DEPRECATED! Use Base.Evaluation.SequentialEvaluator.evaluateRecommendations()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequentialEvaluator: Processed 9001 ( 17.84% ) in 31.56 seconds. Users per second: 285\n",
      "SequentialEvaluator: Processed 20001 ( 39.65% ) in 63.05 seconds. Users per second: 317\n",
      "SequentialEvaluator: Processed 30001 ( 59.47% ) in 93.09 seconds. Users per second: 322\n",
      "SequentialEvaluator: Processed 40001 ( 79.29% ) in 123.57 seconds. Users per second: 324\n",
      "SequentialEvaluator: Processed 50446 ( 100.00% ) in 152.95 seconds. Users per second: 330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.13021646909567566,\n",
       " 'recall': 0.05535860911029997,\n",
       " 'map': 0.07735553623582404}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimbprrecommender.evaluateRecommendations(URM_all, at=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6077014b73934c11afef9b794c5836f3b3da50c"
   },
   "source": [
    "# Generation of the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1afbf49af4c89a7b47718fdb912a4358e648e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:83: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:84: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:85: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Let's generate recommendations for the target playlists\n",
    "\n",
    "print(\"Generating recommendations...\")\n",
    "\n",
    "target_playlist_path = \"../input/target_playlists.csv\"\n",
    "target_playlist_file = open(target_playlist_path, 'r')\n",
    "target_playlist_file.seek(0)\n",
    "target_playlist_tuples = []\n",
    "numberOfTargets = 0\n",
    "\n",
    "for line in target_playlist_file:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    try:\n",
    "        playlist_id = int(line)\n",
    "        target_playlist_tuples.append((playlist_id, list(slimbprrecommender.recommend(playlist_id))))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_description_from_recommendation(tuple):\n",
    "    playlist_string = \"{}\".format(tuple[0])\n",
    "    tracks_string = \"{}\".format(tuple[1]).replace(\", \", \" \").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return \"{},{}\\n\".format(playlist_string, tracks_string)\n",
    "\n",
    "\n",
    "submission_path = \"submission.csv\"\n",
    "submission_file = open(submission_path, 'w')\n",
    "submission_file.write(\"playlist_id,track_ids\\n\")\n",
    "for recommendation in target_playlist_tuples:\n",
    "    submission_file.write(get_description_from_recommendation(recommendation))\n",
    "    numberOfTargets += 1\n",
    "submission_file.close()\n",
    "print(numberOfTargets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "486f73356a2ba5ebdb828dd3cd0928da2c0a9152"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4710325d95ed90d1603dd42c49a666a13f966e33"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ef50a1b3f46413cd96977c874abfd5b4c1fc04f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b87a8f226ea130ab1ead489a9af8aaf82f9e7f9"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
